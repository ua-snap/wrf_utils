{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6581d610-cb64-4ad6-aa3e-5a61cb816a6b",
   "metadata": {},
   "source": [
    "# Compare new restacked data with production data\n",
    "\n",
    "This notebook is for comparing the newest restacked WRF data, created as an upgrade replacement for the existing dataset, with the existing production dataset.\n",
    "It will create a table of comparison results that can be summarized and explored to help avoid erroneously replacing production data with good data.\n",
    "\n",
    "This notebook is NOT expected to maintain functionality, as the existing production dataset could be permanently deleted/archived after ensuring the new data match it where expected. Rather, tables generated using it for each WRF group will serve as historical records for the Great Restacking of 2022, in case they may be useful at a later date.\n",
    "\n",
    "## Strategy\n",
    "\n",
    "This notebook will check consistency by comparing a random slice in time between new outputs and existing production data for every new restacked and resampled file created for a particular WRF group.\n",
    "\n",
    "Given space constraints, it will not be feasible to run a comparison for all WRF groups at once. So this notebook should be executed after the completion of restacking, resampling, and quality checking each of the five WRF groups. Simply run this notebook as a final step before replacing the old production data with new data.\n",
    "\n",
    "Set up directories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21579973-5686-401b-9435-fb5345a17474",
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing.pool import Pool\n",
    "from pathlib import Path\n",
    "import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import luts\n",
    "from config import *\n",
    "# for a type of warning that can occur when comparing times between files\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "\n",
    "# these paths should be constant for any SNAPer running this pipeline\n",
    "# assumes all folders are created in restack_20km.ipynb\n",
    "base_dir = Path(\"/import/SNAP/wrf_data/project_data/wrf_data\")\n",
    "# final output directory for hourly data\n",
    "restack_prod_dir = base_dir.joinpath(\"hourly_fix\")\n",
    "# final output directory for daily data\n",
    "resample_prod_dir = base_dir.joinpath(\"daily\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a51473-4651-441f-90a3-62e378fda0b6",
   "metadata": {},
   "source": [
    "Define a function that will compare randomly selected slices in time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b09e9acf-e665-46fb-be45-fad0a38ec3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_scratch(args):\n",
    "    \"\"\"Run a comparison between a scratch file and a production file. \n",
    "    Test the data values of a single time slice in each of the two restacked files for equivalence, one on scratch space and the corresponding \"production\" file.\n",
    "    \n",
    "    Args:\n",
    "        restack_scratch_fp (path_like): path to file containing restacked data to check that is on scratch space\n",
    "        restack_prod_fp (path_like): path to production file containing restacked data to compare with\n",
    "    \n",
    "    Returns:\n",
    "        dict with keys variable, timestamp, and result as keys\n",
    "    \"\"\"\n",
    "    restack_scratch_fp, restack_prod_fp = args\n",
    "    varname = restack_scratch_fp.parent.name\n",
    "    with xr.open_dataset(restack_scratch_fp) as check_ds:\n",
    "        idx = np.random.randint(check_ds[\"time\"].values.shape[0])\n",
    "        # save time from each because those should match\n",
    "        check_time = check_ds[\"time\"].values[idx]\n",
    "        check_arr = check_ds[varname].sel(time=check_time).values\n",
    "    del check_ds\n",
    "    \n",
    "    try:\n",
    "        with xr.open_dataset(restack_prod_fp) as prod_ds:\n",
    "            # using this dataset's time values in case times don't match (has happened at least once)\n",
    "            prod_time = prod_ds[\"time\"].values[idx]\n",
    "            prod_arr = prod_ds[varname].sel(time=prod_time).values\n",
    "\n",
    "            # checks to see whether all data values in single time slice match between new file and production\n",
    "            arr_result = np.all(prod_arr == check_arr)\n",
    "            prod_exists = True\n",
    "        del prod_ds\n",
    "            \n",
    "    except FileNotFoundError:\n",
    "        # if \"production\" version does not exist, make a note of it\n",
    "        arr_result = False\n",
    "        prod_exists = False\n",
    "        prod_time = np.datetime64(\"2022-01-01T00:00:00\")\n",
    "    \n",
    "    # check to see whether time values match (they should)\n",
    "    time_result = prod_time == check_time\n",
    "    \n",
    "    # wrf_time_str = str(check_time.astype(\"datetime64[h]\")).replace(\"T\", \"_\")\n",
    "    model, scenario = restack_scratch_fp.name.split(\"_\")[-3:-1]\n",
    "    result = {\n",
    "        \"varname\": varname,\n",
    "        \"scratch_filename\": restack_scratch_fp,\n",
    "        \"prod_exists\": prod_exists,\n",
    "        \"timestamp\": check_time,\n",
    "        \"arr_result\": arr_result,\n",
    "        \"time_result\": time_result,\n",
    "    }\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b0f0a86-ec0c-4a26-9206-e32113eda900",
   "metadata": {},
   "source": [
    "### Hourly restacked data\n",
    "\n",
    "Iterate over all new files of the WRF group and generate args for `Pool`-ing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "317fb92e-955f-46c0-96a8-c75f379798d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = []\n",
    "for varname in [v.lower() for v in luts.varnames]:\n",
    "    fn_str = luts.groups[group][\"fn_str\"]\n",
    "    years = luts.groups[group][\"years\"]\n",
    "    for year in years:\n",
    "        fn = f\"{varname}_hourly_wrf_{fn_str}_{year}.nc\"\n",
    "        restack_scratch_fp = hourly_dir.joinpath(varname, fn)\n",
    "        restack_prod_fp = restack_prod_dir.joinpath(varname, fn)\n",
    "        args.append((restack_scratch_fp, restack_prod_fp))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cfd8e24-14d4-4edd-8f3e-ed8fe272eaf6",
   "metadata": {},
   "source": [
    "Run the comparison in parallel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0417056-028b-441f-90fa-9ef96f16cfc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|███████████████████████████████████████████████████████████████████████▎                                | 1135/1656 [55:49<17:04,  1.97s/it]"
     ]
    }
   ],
   "source": [
    "np.random.seed(99709)\n",
    "with Pool(20) as pool:\n",
    "    new_rows = [\n",
    "        result for result in tqdm.tqdm(\n",
    "            pool.imap_unordered(compare_scratch, args), total=len(args))\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ceafcd9-b645-4fdf-b7b3-b52b08b59f55",
   "metadata": {},
   "source": [
    "Store results in a table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256bdf91-268a-4189-9320-c29f22e23f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_results_df = pd.DataFrame(new_rows)\n",
    "hourly_results_fp = anc_dir.joinpath(\n",
    "    \"production_data_comparisons\",\n",
    "    f\"prod_comparison_{luts.groups[group]['fn_str']}_hourly.csv\"\n",
    ")\n",
    "hourly_results_df.to_csv(hourly_results_fp, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ae8798-53d7-4dec-b24b-0b11e419e961",
   "metadata": {},
   "source": [
    "### Daily resampled data\n",
    "\n",
    "Again, iterate over all possible variable names/year combinations to get the new and production daily files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d16760ff-714b-4b07-bc82-7b3590a295b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = []\n",
    "for varname in luts.resample_varnames:\n",
    "    fn_str = luts.groups[group][\"fn_str\"]\n",
    "    years = luts.groups[group][\"years\"]\n",
    "    for year in years:\n",
    "        fn = f\"{varname}_daily_wrf_{fn_str}_{year}.nc\"\n",
    "        resample_scratch_fp = daily_dir.joinpath(varname, fn)\n",
    "        resample_prod_fp = resample_prod_dir.joinpath(varname, fn)\n",
    "        args.append((resample_scratch_fp, resample_prod_fp))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbad032a-0a6d-4a83-abec-b9f6d8a4fa13",
   "metadata": {},
   "source": [
    "And run the comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7e19d47c-e9fa-4198-a926-965163c972af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 216/216 [01:15<00:00,  2.86it/s]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(99709)\n",
    "with Pool(20) as pool:\n",
    "    new_rows = [\n",
    "        result for result in tqdm.tqdm(\n",
    "            pool.imap_unordered(compare_scratch, args), total=len(args))\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "18b366f3-3bfa-43aa-846f-958ab40aa5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_results_df = pd.DataFrame(new_rows)\n",
    "daily_results_fp = anc_dir.joinpath(\n",
    "    \"production_data_comparisons\",\n",
    "    f\"prod_comparison_{luts.groups[group]['fn_str']}_daily.csv\"\n",
    ")\n",
    "daily_results_df.to_csv(daily_results_fp, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "776d1f79-65f2-41ed-affb-1393eeb7cc83",
   "metadata": {},
   "source": [
    "The hourly and daily results may now be investigated to "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
