{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8808d314-7b23-4581-8d0f-6dfb71154736",
   "metadata": {},
   "source": [
    "# 20 km WRF re-stacking pipeline\n",
    "\n",
    "This pipeline restructures the raw 20km WRF outputs that cover Alaska and the surrounding regions (created by P Bieniek) into more user-friendly files that can be easily imported into popular GIS software. This WRF dataset consists of hourly outputs for one reanalysis, ERA-Interim, and two GCMs, GFDL-CM3, and NCAR-CCSM4. This pipeline is designed to be executed entirely from this notebook.\n",
    "\n",
    "This is a rather complicated SNAP data pipeline. It works on a large amount of data (~300 GB for a single model / scenario / year, so that's over 90 TB for $2 * 95 + 2 * 35 + 35$ model / scenario / year combinations), creates a large number of final data files (>10k), and makes use of slurm, specific directory structure / file management, and asyncronous execution ability (i.e. re-run certain steps, run steps for only certain variables, etc). The \"Setup\" step provides info on executing it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af3e39cb-c415-4058-b3d8-0567d8dcf46b",
   "metadata": {},
   "source": [
    "# 0 - Setup\n",
    "\n",
    "This step provides instructions for setting up and running the pipeline. \n",
    "\n",
    "First off, a snapshot of the structure of the target base data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "09e07936-3241-457a-a012-7c030ce89f78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[38;5;27m1970\u001b[0m/\n",
      "\u001b[38;5;27m1971\u001b[0m/\n",
      "\u001b[38;5;27m1972\u001b[0m/\n",
      "\u001b[38;5;27m1973\u001b[0m/\n",
      "\u001b[38;5;27m1974\u001b[0m/\n"
     ]
    }
   ],
   "source": [
    "ls /archive/DYNDOWN/DIONE/pbieniek/ccsm/hist/hourly | head -5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "76100442-3e72-497e-9387-11d9d9c1c7f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;27m2003\u001b[0m/\n",
      "\u001b[38;5;27m2004\u001b[0m/\n",
      "\u001b[38;5;27m2005\u001b[0m/\n",
      "nohup.out\n",
      "\u001b[38;5;34morgdata.sh\u001b[0m*\n",
      "\u001b[m"
     ]
    }
   ],
   "source": [
    "ls /archive/DYNDOWN/DIONE/pbieniek/ccsm/hist/hourly/ | tail -6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "60a75bd5-98b6-42af-8934-c008bafd896d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[38;5;34mdailylog.out\u001b[0m*\n",
      "\u001b[38;5;34mWRFDS_d01.1979-01-01_00.nc\u001b[0m*\n",
      "\u001b[38;5;34mWRFDS_d01.1979-01-01_01.nc\u001b[0m*\n",
      "\u001b[38;5;34mWRFDS_d01.1979-01-01_02.nc\u001b[0m*\n",
      "\u001b[38;5;34mWRFDS_d01.1979-01-01_03.nc\u001b[0m*\n",
      "ls: write error\n"
     ]
    }
   ],
   "source": [
    "ls /archive/DYNDOWN/DIONE/pbieniek/ccsm/hist/hourly/1979 | head -5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ac64b4-6808-48a2-9b07-04f7d5450a51",
   "metadata": {},
   "source": [
    "This structure applies for all outputs, and exists for the following model / scenario / year combinations:\n",
    "\n",
    "* `era/`:\n",
    "    * `hist/`: 1979-2015\n",
    "* `gfdl/`\n",
    "    * `hist/`: 1970-2006\n",
    "    * `rcp85/`: 2006-2100\n",
    "* `ccsm/`\n",
    "    * `hist/`: 1970-2005\n",
    "    * `rcp85/`: 2005-2100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b91d0803-1d11-420c-87cd-e2a68afd5248",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 0.1 - Pipeline execution\n",
    "\n",
    "### Processing\n",
    "\n",
    "The default configuration for this pipeline is to process all available data - all year / variable / model / scenario combinations possible. However, at the finest level of control, this pipeline can re-stack a single year's worth of data for a single variable / model / scenario combination.\n",
    "\n",
    "As seen above, the input data are grouped by model and scenario names and are consistently structured - hourly WRF outputs grouped by yearly folders. Thus, processing is done at the model / scenario \"group\" level - more on that below.\n",
    "\n",
    "Given the large file size / abundance issue, this pipeline is best utilized in an async fashion, with memory management tasks, regular printouts of what's happening and progress on things, what files are where for which groups, etc. \n",
    "\n",
    "### System\n",
    "\n",
    "This pipeline is being developed on the Chinook cluster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09b5fef0-7edc-420f-a9b2-e43d858a7f4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linux chinook00.rcs.alaska.edu 2.6.32-754.35.1.el6.61015g0000.x86_64 #1 SMP Mon Dec 21 12:41:07 EST 2020 x86_64 x86_64 x86_64 GNU/Linux\n"
     ]
    }
   ],
   "source": [
    "!uname -a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553b2acb-eb3a-474f-9ace-0a5cf7132d0f",
   "metadata": {},
   "source": [
    "This pipeline makes use of slurm and multiple cores / compute nodes for processing in reasonable time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "381a1c57-bcad-4f25-8c56-ebf2367b1541",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "slurm 19.05.7\n"
     ]
    }
   ],
   "source": [
    "!sinfo -V"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41bcaca6-33c8-4485-9b69-52ff4a380746",
   "metadata": {},
   "source": [
    "### Execution\n",
    "\n",
    "This notebook should be executed sequentially to process the entire dataset. To process only subsets of the target dataset, which might be done for fixing an issue or re-processing some failed runs, all code cells in this Setup section from Section 0.2 onward need to be executed prior."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98bab88-a2ff-4e64-8978-c4d7c8eb69a1",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 0.2 - Environment\n",
    "\n",
    "Instead of relying on environment variables, this pipeline utilizes user-supplied parameters specified in the cells of this notebook by simply assigning values to variables prior to executing any processing code cells.\n",
    "\n",
    "### 0.2.1 - global parameters\n",
    "\n",
    "The following variables are used throughout the pipeline and are loadset in the code cell below:\n",
    "\n",
    "* `base_dir` - Full path to the directory that will contain all ancillary and intermediate files that will be kept, such as scripts for slurm / `sbatch`\n",
    "* `output_dir` - Full path to the directory that will contain the final output data (will be the same as `base_dir` here but specified separately for consistency with other SNAP pipelines)\n",
    "* `scratch_dir` - Full path to the scratch directory that raw WRF outputs will be copied to prior to processing them\n",
    "    * This pipelines works with WRF outputs that are on a mounted file system, and so can be copied over to scratch space and removed when done to improve IO and avoid the need to keep them in the `base_dir`.\n",
    "* `slurm_email` - String containing email address to use for failed slurm notifications\n",
    "* `conda_init_script` - This is currently specific to Chinook. This is the path to a script that contains commands for initializing the shells on the compute nodes to use `conda activate`, has the typical commands seen in `~/.bashrc` after installing conda:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "841e47c7-1503-4d1b-9ccc-f8babce63dba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/bin/bash\n",
      "\n",
      "# >>> conda initialize >>>\n",
      "# !! Contents within this block are managed by 'conda init' !!\n",
      "__conda_setup=\"$('/home/kmredilla/miniconda3/bin/conda' 'shell.bash' 'hook' 2> /dev/null)\"\n",
      "if [ $? -eq 0 ]; then\n",
      "    eval \"$__conda_setup\"\n",
      "else\n",
      "    if [ -f \"/home/kmredilla/miniconda3/etc/profile.d/conda.sh\" ]; then\n",
      "        . \"/home/kmredilla/miniconda3/etc/profile.d/conda.sh\"\n",
      "    else\n",
      "        export PATH=\"/home/kmredilla/miniconda3/bin:$PATH\"\n",
      "    fi\n",
      "fi\n",
      "unset __conda_setup\n",
      "# <<< conda initialize <<< \n",
      "\n"
     ]
    }
   ],
   "source": [
    "cat ~/init_conda.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "608535b5-a88c-424b-9c1f-90e937526bb9",
   "metadata": {},
   "source": [
    "Supply the values for these parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd782b68-59a3-4154-97dd-63f7bf303b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# User-parameters\n",
    "base_dir = \"/import/SNAP/wrf_data/project_data/wrf_data\"\n",
    "output_dir = \"/import/SNAP/wrf_data/project_data/wrf_data\"\n",
    "scratch_dir = \"/center1/DYNDOWN/kmredilla/wrf_data\"\n",
    "slurm_email = \"kmredilla@alaska.edu\"\n",
    "conda_init_script = \"/home/kmredilla/init_conda.sh\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee5f413a-b0ae-4c94-9f4e-61a95b42c01f",
   "metadata": {},
   "source": [
    "### 0.2.2 - job parameters\n",
    "\n",
    "The following arguments are required for a single job of re-stacking data for a particular variable (or variables), model, scenario, and year (or years):\n",
    "\n",
    "* `varname`: Name of the variable. This is the lower case version of the variable name in the WRF outputs.\n",
    "* `wrf_dir`: This is the directory containing the WRF files. This codebase is designed for use with hourly output, so this needs to be the `hourly/` directory if there are multiple options (e.g. `daily/`, `monthly/`, etc.).\n",
    "* `group`: Encoded value specifying the WRF group being worked on, which is just a combination of the model and scenario (or just model, in terms of ERA-Interim).  One of [`era_interim`, `gfdl_hist`, `ccsm_hist`, `gfdl_rcp85`, `ccsm_rcp85`].\n",
    "* `years`: a list of years to work on specified as integers, such as `[1979, 1980]`, or omit to work on all years available for a given WRF group.\n",
    "\n",
    "The WRF outputs of interest from different runs of model/scenario may be in separate places, but there is consistency in file structure across all groups - all `hourly` directories have annual subgroups consisting of the WRF outputs to be restacked."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b705d6-6b4a-492c-a474-63769b8211c1",
   "metadata": {},
   "source": [
    "## 0.3 - Global imports and filepaths\n",
    "\n",
    "Set up all filepathing used in the cell below and import all packages used in multiple sections. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9414518-e855-49c0-8965-4e5fbddb14cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from pathlib import Path\n",
    "# codebase\n",
    "import luts\n",
    "import restack_20km as main\n",
    "\n",
    "base_dir = Path(\"/import/SNAP/wrf_data/project_data/wrf_data\")\n",
    "output_dir = Path(\"/import/SNAP/wrf_data/project_data/wrf_data\")\n",
    "scratch_dir = Path(\"/center1/DYNDOWN/kmredilla/wrf_data\")\n",
    "# where raw wrf outputs will be copied on scratch\n",
    "raw_scratch_dir = scratch_dir.joinpath(\"raw\")\n",
    "raw_scratch_dir.mkdir(exist_ok=True)\n",
    "# where initially restacked data will be stored on scratch_space\n",
    "restack_scratch_dir = scratch_dir.joinpath(\"restacked\")\n",
    "restack_scratch_dir.mkdir(exist_ok=True)\n",
    "\n",
    "slurm_dir = scratch_dir.joinpath(\"slurm\")\n",
    "slurm_dir.mkdir(exist_ok=True)\n",
    "slurm_email = \"kmredilla@alaska.edu\"\n",
    "\n",
    "# this env is always defined if notebook started with anaconda-project run\n",
    "project_dir = Path(os.getenv(\"PROJECT_DIR\"))\n",
    "ap_env = project_dir.joinpath(\"envs/default\")\n",
    "cp_script = project_dir.joinpath(\"restack_20km/mp_cp.py\")\n",
    "restack_script = project_dir.joinpath(\"restack.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "403f4d6c-88d3-4733-bf43-d365efbf23e8",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 1 - Re-stack data and improve the file structure\n",
    "\n",
    "This is the main lift of the pipeline and it applies to a single WRF group, for any variables and years specified. It re-stacks the WRF outputs, which means extracting the data for all variables in a single WRF file and combining them into new files grouped by variable and year. It then assigns useful metadata and restructures the files to achieve greater usability (note - this was previously a separate step, but the storage of essentially duplicate intermediate data was not efficient).\n",
    "\n",
    "As mentioned above, this pipeline is currently configured to run for all potential combinations of variables / years for each group. This section will demonstrate execution of all the processing steps required to re-stack one single WRF group, NCAR-CCSM4 historical, and then will proceed to string them all together for processing the remaining WRF groups."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ebb316-9827-4495-bb0a-996d59194f14",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1.1 - Copy WRF data to scratch space \n",
    "\n",
    "If not present on the filesystem (as is the at the time of developing the current code) then the WRF data need to be copied over. \n",
    "\n",
    "This step will copy the annual subdirectory (or directories) containing the WRF outputs for all specified years to scratch space for efficient reading. This step utilizes `sbatch`.\n",
    "\n",
    "Specify the desired job parameters in the code cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0256af0-bd62-4d3f-8c1b-e2381439586f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# job parameters\n",
    "wrf_dir = Path(\"/archive/DYNDOWN/DIONE/pbieniek/ccsm/hist/hourly\")\n",
    "group = \"ccsm_hist\"\n",
    "years = [2004, 2005]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "298c66a2-a2b7-47eb-9ff1-00575bf8b04a",
   "metadata": {},
   "source": [
    "Use slurm to breakup the work for copying multiple years across nodes. Specify the number of CPUs to use in the `ncpus` parameter and write the `sbatch` scripts for copying the data for each year:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2331bb4f-53cf-4ab9-ae3b-bef68310be54",
   "metadata": {},
   "outputs": [],
   "source": [
    "ncpus = 10\n",
    "partition = \"t1small\"\n",
    "\n",
    "# if no years supplied, get all\n",
    "if len(years) == 0:\n",
    "    years = luts.groups[group][\"years\"]\n",
    "\n",
    "sbatch_fps = []\n",
    "group_dir = raw_scratch_dir.joinpath(group)\n",
    "for year in years:\n",
    "    # write to .slurm script\n",
    "    sbatch_fp = slurm_dir.joinpath(f\"cp_scratch_{group}_{year}.slurm\")\n",
    "    # filepath for slurm stdout\n",
    "    sbatch_out_fp = slurm_dir.joinpath(f\"cp_scratch_%j_{group}_{year}.out\")\n",
    "    src_dir = wrf_dir.joinpath(str(year))\n",
    "    dst_dir = group_dir.joinpath(str(year))\n",
    "    sbatch_head = main.make_sbatch_head(ncpus, slurm_email, partition, conda_init_script, ap_env)\n",
    "    main.write_sbatch_copyto_scratch(sbatch_fp, sbatch_out_fp, src_dir, dst_dir, cp_script, ncpus, sbatch_head)\n",
    "    sbatch_fps.append(sbatch_fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0962237-4875-42a0-84c1-6f39cffd6895",
   "metadata": {},
   "source": [
    "Ensure yearly subdirectories are present before submitting the batch jobs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6a0986b7-e3b2-4d26-befc-77fcd2d751a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "main.make_yearly_scratch_dirs(group, years, raw_scratch_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "380d6da3-2621-4e9b-9d87-3d8e64d8aefb",
   "metadata": {},
   "source": [
    "Make sure the directories on `$ARCHIVE` have been staged for copying over, e.g.:\n",
    "\n",
    "```\n",
    "batch_stage -r /archive/DYNDOWN/DIONE/pbieniek/ccsm/hist/hourly\n",
    "```\n",
    "\n",
    "And then call `sbatch` to submit all of the `sbatch_fps`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa69830-a0ab-4015-b408-ec3b7c0d6649",
   "metadata": {},
   "outputs": [],
   "source": [
    "# job_ids = [main.submit_sbatch(fp) for fp in sbatch_fps]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f5b910-3c72-4cfa-b750-80c493cca5f3",
   "metadata": {},
   "source": [
    "#### Check progress of copying to scratch\n",
    "\n",
    "Run the cell below to check the progress of the copy for the current arguments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "017805fb-4dd1-4104-8ed7-e5cee73814ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 of 17520 requestedWRF output files found in /center1/DYNDOWN/kmredilla/wrf_data/raw.\n"
     ]
    }
   ],
   "source": [
    "_ = main.check_raw_scratch(wrf_dir, group, years, raw_scratch_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3095096-50c5-4d59-a4e3-9e5ae87c409c",
   "metadata": {},
   "source": [
    "## 1.2: Restack the data\n",
    "\n",
    "Now that the WRF outputs are available on the scratch filesystem for faster access, execute the restacking script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0edd91-cc59-4694-a536-1047aa4149a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how to ensure files aren't opened at same time by different processes? Is that bad anyway? Is there handling for that currently?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e22dfa3-6e1f-449e-8bac-fd1244f5cbd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sbatch_restack(group, year, variable):\n",
    "    \"\"\"Restack the WRF outputs by creating and \n",
    "    submitting a sbatch job for a single group,\n",
    "    year, and variable\n",
    "    \"\"\"\n",
    "\n",
    "    restack_script = Path(os.getcwd()).joinpath(\"restack.py\")\n",
    "    \n",
    "    slurm_dir = base_dir.joinpath(\"slurm/restack\")\n",
    "    slurm_dir.mkdir(exist_ok=True, parents=True)\n",
    "    \n",
    "    # setup command\n",
    "    sbatch_out_str = f\"{str(slurm_dir)}/slurm_restack_%j_{group}_{year}.out\"\n",
    "    head = (\n",
    "        \"#!/bin/sh\\n\"\n",
    "        \"#SBATCH --nodes=1\\n\"\n",
    "        f\"#SBATCH --cpus-per-task={ncpus}\\n\"\n",
    "        \"#SBATCH --account=snap\\n\"\n",
    "        \"#SBATCH --mail-type=FAIL\\n\"\n",
    "        f\"#SBATCH --mail-user={slurm_email}\\n\"\n",
    "        \"#SBATCH -p main\\n\"\n",
    "        f'#SBATCH --output {sbatch_out_str}\\n'\n",
    "        'eval \"$(conda shell.bash hook)\"\\n'\n",
    "        # base conda env has anaconda-project installed\n",
    "        \"conda activate\\n\"\n",
    "    )\n",
    "    command = f\"anaconda-project run python {restack_script} -fp {} -n {ncpus}\\n\"\n",
    "    \n",
    "    # write to .slurm script\n",
    "    sbatch_fp = slurm_dir.joinpath(pairs_fp.name.replace(\".pickle\", \".slurm\"))\n",
    "    with open(sbatch_fp, \"w\") as f:\n",
    "        f.write(head + command)\n",
    "        \n",
    "    out = subprocess.check_output([\"sbatch\", sbatch_fp])\n",
    "    \n",
    "    return {\"subprocess_out\": out, \"sbatch_out_str\": sbatch_out_str}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776a41dc-4753-4678-9c7a-3139e3d3a166",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b88060d-827b-48a2-bde9-505f78e0e980",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "384c3f7a-ccaf-4abd-81d2-8a8a938d157f",
   "metadata": {},
   "source": [
    "### 1.3 Move stacked from scratch\n",
    "\n",
    "Should this step be kept?? Could run improve step while on scratch as well..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79cfd858-1407-4975-bff6-995dbbefe1aa",
   "metadata": {},
   "source": [
    "### 1.4 Remove the WRF outputs from scratch\n",
    "\n",
    "To clear up space in a $SCRATCH_DIR with limited capacity, remove the WRF outputs that have been completed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d775dc13-e349-479a-992f-d161b4f29ca0",
   "metadata": {},
   "source": [
    "## 2: Improve the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e032942a-c795-48b3-ba65-3f477f2431a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "group_dir.joinpath(str(year)).mkdir(exist_ok=True, parents=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0038dfaf-4094-41f9-ac48-e32e1b4bc386",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hist'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrf_dir.parent.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "012f6d1c-9455-4a8e-8688-9c49d6d6450e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = []\n",
    "test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
