{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8808d314-7b23-4581-8d0f-6dfb71154736",
   "metadata": {},
   "source": [
    "# 20 km WRF re-stacking pipeline\n",
    "\n",
    "This pipeline restructures the raw 20km WRF outputs that cover Alaska and the surrounding regions (created by P Bieniek) into more user-friendly files that can be easily imported into popular GIS software. This WRF dataset consists of hourly outputs for one reanalysis, ERA-Interim, and two GCMs, GFDL-CM3, and NCAR-CCSM4. This pipeline is designed to be executed entirely from this notebook.\n",
    "\n",
    "This is a rather complicated SNAP data pipeline. It works on a large amount of data (~300 GB for a single model / scenario / year, so that's over 90 TB for $2 * 95 + 2 * 35 + 35$ model / scenario / year combinations), creates a large number of final data files (>10k), and makes use of slurm, specific directory structure / file management, and asyncronous execution ability (i.e. re-run certain steps, run steps for only certain variables, etc). The \"Setup\" step provides info on executing it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af3e39cb-c415-4058-b3d8-0567d8dcf46b",
   "metadata": {},
   "source": [
    "# 0 - Setup\n",
    "\n",
    "This step provides instructions for setting up and running the pipeline. \n",
    "\n",
    "First off, a snapshot of the structure of the target base data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "09e07936-3241-457a-a012-7c030ce89f78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[38;5;27m1970\u001b[0m/\n",
      "\u001b[38;5;27m1971\u001b[0m/\n",
      "\u001b[38;5;27m1972\u001b[0m/\n",
      "\u001b[38;5;27m1973\u001b[0m/\n",
      "\u001b[38;5;27m1974\u001b[0m/\n"
     ]
    }
   ],
   "source": [
    "ls /archive/DYNDOWN/DIONE/pbieniek/ccsm/hist/hourly | head -5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "76100442-3e72-497e-9387-11d9d9c1c7f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;27m2003\u001b[0m/\n",
      "\u001b[38;5;27m2004\u001b[0m/\n",
      "\u001b[38;5;27m2005\u001b[0m/\n",
      "nohup.out\n",
      "\u001b[38;5;34morgdata.sh\u001b[0m*\n",
      "\u001b[m"
     ]
    }
   ],
   "source": [
    "ls /archive/DYNDOWN/DIONE/pbieniek/ccsm/hist/hourly/ | tail -6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "60a75bd5-98b6-42af-8934-c008bafd896d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[38;5;34mdailylog.out\u001b[0m*\n",
      "\u001b[38;5;34mWRFDS_d01.1979-01-01_00.nc\u001b[0m*\n",
      "\u001b[38;5;34mWRFDS_d01.1979-01-01_01.nc\u001b[0m*\n",
      "\u001b[38;5;34mWRFDS_d01.1979-01-01_02.nc\u001b[0m*\n",
      "\u001b[38;5;34mWRFDS_d01.1979-01-01_03.nc\u001b[0m*\n",
      "ls: write error\n"
     ]
    }
   ],
   "source": [
    "ls /archive/DYNDOWN/DIONE/pbieniek/ccsm/hist/hourly/1979 | head -5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ac64b4-6808-48a2-9b07-04f7d5450a51",
   "metadata": {},
   "source": [
    "This structure applies for all outputs, and exists for the following model / scenario / year combinations:\n",
    "\n",
    "* `era/`:\n",
    "    * `hist/`: 1979-2015\n",
    "* `gfdl/`\n",
    "    * `hist/`: 1970-2006\n",
    "    * `rcp85/`: 2006-2100\n",
    "* `ccsm/`\n",
    "    * `hist/`: 1970-2005\n",
    "    * `rcp85/`: 2005-2100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b91d0803-1d11-420c-87cd-e2a68afd5248",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 0.1 - Pipeline execution\n",
    "\n",
    "### Processing\n",
    "\n",
    "The default configuration for this pipeline is to process all available data - all year / variable / model / scenario combinations possible. However, at the finest level of control, this pipeline can re-stack a single year's worth of data for a single variable / model / scenario combination.\n",
    "\n",
    "As seen above, the input data are grouped by model and scenario names and are consistently structured - hourly WRF outputs grouped by yearly folders. Thus, processing is done at the model / scenario \"group\" level - more on that below.\n",
    "\n",
    "Given the large file size / abundance issue, this pipeline is best utilized in an async fashion, with memory management tasks, regular printouts of what's happening and progress on things, what files are where for which groups, etc. \n",
    "\n",
    "### System\n",
    "\n",
    "This pipeline is being developed on the Chinook cluster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09b5fef0-7edc-420f-a9b2-e43d858a7f4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linux chinook00.rcs.alaska.edu 2.6.32-754.35.1.el6.61015g0000.x86_64 #1 SMP Mon Dec 21 12:41:07 EST 2020 x86_64 x86_64 x86_64 GNU/Linux\n"
     ]
    }
   ],
   "source": [
    "!uname -a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553b2acb-eb3a-474f-9ace-0a5cf7132d0f",
   "metadata": {},
   "source": [
    "This pipeline makes use of slurm and multiple cores / compute nodes for processing in reasonable time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "381a1c57-bcad-4f25-8c56-ebf2367b1541",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "slurm 19.05.7\n"
     ]
    }
   ],
   "source": [
    "!sinfo -V"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41bcaca6-33c8-4485-9b69-52ff4a380746",
   "metadata": {},
   "source": [
    "### Execution\n",
    "\n",
    "This notebook should be executed sequentially to process the entire dataset. To process only subsets of the target dataset, which might be done for fixing an issue or re-processing some failed runs, all code cells in this Setup section from Section 0.2 onward need to be executed prior."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98bab88-a2ff-4e64-8978-c4d7c8eb69a1",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 0.2 - Environment\n",
    "\n",
    "Instead of relying on environment variables, this pipeline utilizes user-supplied parameters specified in the cells of this notebook by simply assigning values to variables prior to executing any processing code cells.\n",
    "\n",
    "### 0.2.1 - global parameters\n",
    "\n",
    "The following variables are used throughout the pipeline and are loadset in the code cell below:\n",
    "\n",
    "* `base_dir` - Full path to the directory that will contain all ancillary and intermediate files that will be kept, such as scripts for slurm / `sbatch`\n",
    "* `output_dir` - Full path to the directory that will contain the final output data (will be the same as `base_dir` here but specified separately for consistency with other SNAP pipelines)\n",
    "* `scratch_dir` - Full path to the scratch directory that raw WRF outputs will be copied to prior to processing them\n",
    "    * This pipelines works with WRF outputs that are on a mounted file system, and so can be copied over to scratch space and removed when done to improve IO and avoid the need to keep them in the `base_dir`.\n",
    "* `slurm_email` - String containing email address to use for failed slurm notifications\n",
    "* `conda_init_script` - This is currently specific to Chinook. This is the path to a script that contains commands for initializing the shells on the compute nodes to use `conda activate`, has the typical commands seen in `~/.bashrc` after installing conda:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "841e47c7-1503-4d1b-9ccc-f8babce63dba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/bin/bash\n",
      "\n",
      "# >>> conda initialize >>>\n",
      "# !! Contents within this block are managed by 'conda init' !!\n",
      "__conda_setup=\"$('/home/kmredilla/miniconda3/bin/conda' 'shell.bash' 'hook' 2> /dev/null)\"\n",
      "if [ $? -eq 0 ]; then\n",
      "    eval \"$__conda_setup\"\n",
      "else\n",
      "    if [ -f \"/home/kmredilla/miniconda3/etc/profile.d/conda.sh\" ]; then\n",
      "        . \"/home/kmredilla/miniconda3/etc/profile.d/conda.sh\"\n",
      "    else\n",
      "        export PATH=\"/home/kmredilla/miniconda3/bin:$PATH\"\n",
      "    fi\n",
      "fi\n",
      "unset __conda_setup\n",
      "# <<< conda initialize <<< \n",
      "\n"
     ]
    }
   ],
   "source": [
    "cat ~/init_conda.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "608535b5-a88c-424b-9c1f-90e937526bb9",
   "metadata": {},
   "source": [
    "Supply the values for these parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd782b68-59a3-4154-97dd-63f7bf303b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# User-parameters\n",
    "base_dir = \"/import/SNAP/wrf_data/project_data/wrf_data\"\n",
    "output_dir = \"/import/SNAP/wrf_data/project_data/wrf_data\"\n",
    "scratch_dir = \"/center1/DYNDOWN/kmredilla/wrf_data\"\n",
    "slurm_email = \"kmredilla@alaska.edu\"\n",
    "conda_init_script = \"/home/kmredilla/init_conda.sh\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee5f413a-b0ae-4c94-9f4e-61a95b42c01f",
   "metadata": {},
   "source": [
    "### 0.2.2 - job parameters\n",
    "\n",
    "The following arguments are required for a single job of re-stacking data for a particular variable (or variables), model, scenario, and year (or years):\n",
    "\n",
    "* `varname`: Name of the variable. This is the lower case version of the variable name in the WRF outputs.\n",
    "* `wrf_dir`: This is the directory containing the WRF files. This codebase is designed for use with hourly output, so this needs to be the `hourly/` directory if there are multiple options (e.g. `daily/`, `monthly/`, etc.).\n",
    "* `group`: Encoded value specifying the WRF group being worked on, which is just a combination of the model and scenario (or just model, in terms of ERA-Interim).  One of [`era_interim`, `gfdl_hist`, `ccsm_hist`, `gfdl_rcp85`, `ccsm_rcp85`].\n",
    "* `years`: a list of years to work on specified as integers, such as `[1979, 1980]`, or omit to work on all years available for a given WRF group.\n",
    "\n",
    "The WRF outputs of interest from different runs of model/scenario may be in separate places, but there is consistency in file structure across all groups - all `hourly` directories have annual subgroups consisting of the WRF outputs to be restacked."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b705d6-6b4a-492c-a474-63769b8211c1",
   "metadata": {},
   "source": [
    "## 0.3 - Global imports and filepaths\n",
    "\n",
    "Set up all filepathing used in the cell below and import all packages used in multiple sections. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9414518-e855-49c0-8965-4e5fbddb14cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from multiprocessing import Pool\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm\n",
    "# codebase\n",
    "import luts\n",
    "import restack_20km as main\n",
    "\n",
    "base_dir = Path(\"/import/SNAP/wrf_data/project_data/wrf_data\")\n",
    "anc_dir = base_dir.joinpath(\"ancillary\")\n",
    "anc_dir.mkdir(exist_ok=True)\n",
    "# monthly WRF file to serve as template\n",
    "template_fp = anc_dir.joinpath(\"monthly_PCPT-gfdlh.nc\")\n",
    "# WRF geogrid file for correctly projecting data and rotating wind data\n",
    "geogrid_fp = anc_dir.joinpath(\"geo_em.d01.nc\")\n",
    "# final output directory for data\n",
    "output_dir = Path(\"/import/SNAP/wrf_data/project_data/wrf_data/restacked\")\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "# scratch space where data will be copied for performant reading / writing\n",
    "scratch_dir = Path(\"/center1/DYNDOWN/kmredilla/wrf_data\")\n",
    "# where raw wrf outputs will be copied on scratch\n",
    "raw_scratch_dir = scratch_dir.joinpath(\"raw\")\n",
    "raw_scratch_dir.mkdir(exist_ok=True)\n",
    "# where initially restacked data will be stored on scratch_space\n",
    "restack_scratch_dir = scratch_dir.joinpath(\"restacked\")\n",
    "restack_scratch_dir.mkdir(exist_ok=True)\n",
    "\n",
    "slurm_dir = base_dir.joinpath(\"slurm\")\n",
    "slurm_dir.mkdir(exist_ok=True)\n",
    "slurm_email = \"kmredilla@alaska.edu\"\n",
    "\n",
    "# this env is always defined if notebook started with anaconda-project run\n",
    "project_dir = Path(os.getenv(\"PROJECT_DIR\"))\n",
    "ap_env = project_dir.joinpath(\"envs/default\")\n",
    "# cp_script = project_dir.joinpath(\"restack_20km/mp_cp.py\") not used on Chinook, $ARCHIVE not accessible from compute nodes\n",
    "restack_script = project_dir.joinpath(\"restack_20km/restack.py\")\n",
    "forecast_times_script = project_dir.joinpath(\"restack_20km/forecast_times.py\")\n",
    "luts_fp = project_dir.joinpath(\"restack_20km/luts.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "403f4d6c-88d3-4733-bf43-d365efbf23e8",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 1 - Re-stack data and improve the file structure\n",
    "\n",
    "This is the main lift of the pipeline and it applies to a single WRF group, for any variables and years specified. It re-stacks the WRF outputs, which means extracting the data for all variables in a single WRF file and combining them into new files grouped by variable and year. It then assigns useful metadata and restructures the files to achieve greater usability (note - this was previously a separate step, but the storage of essentially duplicate intermediate data was not efficient).\n",
    "\n",
    "As mentioned above, this pipeline is currently configured to run for all potential combinations of variables / years for each group. This section will demonstrate execution of all the processing steps required to re-stack one single WRF group, NCAR-CCSM4 historical, and then will proceed to string them all together for processing the remaining WRF groups."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ebb316-9827-4495-bb0a-996d59194f14",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1.1 - Copy WRF data to scratch space \n",
    "\n",
    "If not present on the filesystem (as is the at the time of developing the current code) then the WRF data need to be copied over.\n",
    "\n",
    "This step will copy the annual subdirectory (or directories) containing the WRF outputs for all specified years to scratch space for efficient reading. Given the location of the source gest data on $ARCHIVE, which requires files to be brought back online from tape to read them, this step is very time consuming.\n",
    "\n",
    "It takes a very long to to `batch_stage` an entire group directory. This does not make for a good experience when trying to execute this pipeline. Given that this transfer needs to be initiated from a login node (for two reasons: 1) probably not possible to port-forward login nodes and  2) more importantly, $ARCHIVE is only visible to the login nodes). It is best to be able to have commands that 1) what files (years) are missing from the scratch space 2) checks to see what of those necessary files (years) are currently offline 3) copies files (years) that are online \n",
    "\n",
    "Really, the process we want to execute is `sbatch_stage` a year, \n",
    "\n",
    "but performing the copy without so it seems that the best\n",
    "\n",
    "Specify the desired job parameters in the code cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0256af0-bd62-4d3f-8c1b-e2381439586f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# job parameters\n",
    "wrf_dir = Path(\"/archive/DYNDOWN/DIONE/pbieniek/ccsm/hist/hourly\")\n",
    "group = \"ccsm_hist\"\n",
    "# years = [2004, 2005]\n",
    "# to specify all years:\n",
    "years = luts.groups[group][\"years\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "298c66a2-a2b7-47eb-9ff1-00575bf8b04a",
   "metadata": {},
   "source": [
    "Use slurm to breakup the work for copying multiple years across nodes. Specify the number of CPUs to use in the `ncpus` parameter and write the `sbatch` scripts for copying the data for each year:\n",
    "\n",
    "**Note** - The strategy in the below code cell is pointless for the case where source data is on $ARCHVIE on Chinook because compute nodes can't see it! Hence, it is commented out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2331bb4f-53cf-4ab9-ae3b-bef68310be54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ncpus = 10\n",
    "# partition = \"t1small\"\n",
    "\n",
    "# sbatch_fps = []\n",
    "# group_dir = raw_scratch_dir.joinpath(group)\n",
    "# for year in years:\n",
    "#     # write to .slurm script\n",
    "#     sbatch_fp = slurm_dir.joinpath(f\"cp_scratch_{group}_{year}.slurm\")\n",
    "#     # filepath for slurm stdout\n",
    "#     sbatch_out_fp = slurm_dir.joinpath(f\"cp_scratch_{group}_{year}_%j.out\")\n",
    "#     src_dir = wrf_dir.joinpath(str(year))\n",
    "#     dst_dir = group_dir.joinpath(str(year))\n",
    "#     sbatch_head = main.make_sbatch_head(ncpus, slurm_email, partition, conda_init_script, ap_env)\n",
    "#     main.write_sbatch_copyto_scratch(sbatch_fp, sbatch_out_fp, src_dir, dst_dir, cp_script, ncpus, sbatch_head)\n",
    "#     sbatch_fps.append(sbatch_fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "380d6da3-2621-4e9b-9d87-3d8e64d8aefb",
   "metadata": {},
   "source": [
    "Make sure the directories on `$ARCHIVE` have been staged for copying over, e.g.:\n",
    "\n",
    "```\n",
    "batch_stage -r /archive/DYNDOWN/DIONE/pbieniek/ccsm/hist/hourly\n",
    "```\n",
    "\n",
    "Use the cells below to stage the years to be copied. Once this command finishes, then proceed to copying the files.\n",
    "\n",
    "Specify the years to be staged:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "173514e9-ea41-4cf3-9c82-0b9b8975487d",
   "metadata": {},
   "outputs": [],
   "source": [
    "years = list(range(1984, 2004))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2cd71fa-afed-4ad8-a8e4-b8731f723c56",
   "metadata": {},
   "source": [
    "Then execute the cell below to iterate over all years and stage them:\n",
    "\n",
    "(this can take a while)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d411f69-99ed-4928-97f5-3d5d786d449c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from subprocess import check_output\n",
    "\n",
    "\n",
    "stdout = []\n",
    "for year in years:\n",
    "    stage_dir = wrf_dir.joinpath(str(year))\n",
    "    out = check_output([\"batch_stage\", \"-r\", stage_dir])\n",
    "    stdout.append(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8823fcdd-c823-4a3f-ace0-b8f4d325e72f",
   "metadata": {},
   "source": [
    "**Note** - This step should always be done if the data are on `$ARCHIVE` on Chinook, as it should be much more efficient that running the copy without staged (> 3 hours to do a year without staging, ~45 minutes when staged (using 20 cores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa69830-a0ab-4015-b408-ec3b7c0d6649",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this step commented out given configuration of dev machine / cluster\n",
    "# job_ids = [main.submit_sbatch(fp) for fp in sbatch_fps]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c6c050-e596-459b-a87d-970ac95b8ef5",
   "metadata": {},
   "source": [
    "Ensure yearly subdirectories are present before starting the copying:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a0986b7-e3b2-4d26-befc-77fcd2d751a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "main.make_yearly_scratch_dirs(group, years, raw_scratch_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ffaf2f-b04e-4254-907b-c49889b88a8d",
   "metadata": {},
   "source": [
    "Then iterate over years and copy the files in parallel with Pool.\n",
    "\n",
    "Again, specify the years to copy first, then execute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ec6e2591-da13-490f-9662-de9add8bae9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "years = range(1984, 2004)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b6ad9d4c-6fae-4c35-8062-213fcc7796e5",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4ff6a4835574ac7955576d608a9247c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Copying files for 20 years:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88921a296189425380a013e6e43ae7f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Year: 1984:   0%|          | 0/8760 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7db1276b800d4311adf34c728b977c47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Year: 1985:   0%|          | 0/8760 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94bf4df13c2b4bef822cbfa67676f7d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Year: 1986:   0%|          | 0/8760 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b87973fccd7b41489de185e17f037c01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Year: 1987:   0%|          | 0/8760 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87f3d00c282248d2bfefc6a7f7a100f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Year: 1988:   0%|          | 0/8760 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c191d955970c4e06a83d862ab0818182",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Year: 1989:   0%|          | 0/8760 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "042b1eef7fb84e429fc2daa2c9584114",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Year: 1990:   0%|          | 0/8760 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec3383bdbc6f4ec1bcc3cbb5f6573192",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Year: 1991:   0%|          | 0/8760 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73996865286d4addb37e1a5ad2322d8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Year: 1992:   0%|          | 0/8760 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3843b0af4b2c40439e5ca417c50bde4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Year: 1993:   0%|          | 0/8760 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d677315004d45e5877bbed00f4f84d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Year: 1994:   0%|          | 0/8760 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d52f1447055745ff92bd33633bba25fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Year: 1995:   0%|          | 0/8760 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "129640ca5caa49a59cee4df41db44518",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Year: 1996:   0%|          | 0/8760 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f225873d73c43cabc5395495ce37469",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Year: 1997:   0%|          | 0/8760 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "900d100554b94d42abc8791561d92fca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Year: 1998:   0%|          | 0/8760 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f3d1f07a6284414bcf3d6111c8fb076",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Year: 1999:   0%|          | 0/8760 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a95738288a04648b2a1cb31658cc9b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Year: 2000:   0%|          | 0/8760 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8410e94e25ea47aca80e22ae0826ade5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Year: 2001:   0%|          | 0/8760 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50db7453f7f94fb6af4a65ed7ba3d0ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Year: 2002:   0%|          | 0/8760 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b591025f5bc4e89bdf04ef20f6d96ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Year: 2003:   0%|          | 0/8760 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ncpus = 20\n",
    "clobber = \"all\"\n",
    "\n",
    "\n",
    "group_dir = raw_scratch_dir.joinpath(group)\n",
    "for year in tqdm(years, total=len(years), desc=f\"Copying files for {len(years)} years\"):\n",
    "    src_dir = wrf_dir.joinpath(str(year))\n",
    "    dst_dir = group_dir.joinpath(str(year))\n",
    "    # set third arg to False for no-clobber\n",
    "    args = [(fp, dst_dir.joinpath(fp.name), clobber) for fp in src_dir.glob(\"*.nc\")]\n",
    "    \n",
    "    with Pool(ncpus) as pool:\n",
    "        out = [out for out in tqdm(pool.imap(main.sys_copy, args), total=len(args), desc=f\"Year: {year}\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f5b910-3c72-4cfa-b750-80c493cca5f3",
   "metadata": {},
   "source": [
    "#### Check progress of copying to scratch\n",
    "\n",
    "Run the cell below to check the progress of the copy for the current arguments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "017805fb-4dd1-4104-8ed7-e5cee73814ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pooling done..\n",
      "Years with all files present on scratch space: []\n",
      "Years with files missing from scratch space: [1981]\n"
     ]
    }
   ],
   "source": [
    "# modify this to show what entire years are present in scratch_dir\n",
    "\n",
    "wrf_fps, existing_scratch_fps = main.check_raw_scratch(wrf_dir, group, [1981], raw_scratch_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3659ed14-df71-462d-91e9-295d0a9a096f",
   "metadata": {},
   "source": [
    "This cell below can be used as a quick check to identify any files that didn't copy properly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "39b32e46-b334-407b-abe2-3af75eeac97c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "flag_fps = []\n",
    "\n",
    "for year in years:\n",
    "    year_scratch_dir = raw_scratch_dir.joinpath(group, year)\n",
    "    flag_fps.extend(check_scratch_file_sizes(year_scratch_dir, ncpus=20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787302a3-70fb-4ef8-8d8c-ce9df79d4731",
   "metadata": {},
   "source": [
    "Then this cell can be used to re-copy any missing files derived from that check.\n",
    "\n",
    "**Note** - if there is a large number of missing files, it might be more efficient to use the intial section above for copying in batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b109d9bf-8206-4f16-b730-0215310ba779",
   "metadata": {},
   "outputs": [],
   "source": [
    "main.recopy_raw_scratch_files(flag_fps, wrf_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a25943f1-9c55-48a7-a153-58b94cd8ba26",
   "metadata": {},
   "source": [
    "## 1.2 - Restack the data\n",
    "\n",
    "Now that the WRF outputs are available on the scratch filesystem for faster access, execute the restacking script on all variables of interest."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f2b1abb-4aea-4627-aea6-6ab418f79e7f",
   "metadata": {},
   "source": [
    "### 1.2.1 - Make forecast time table\n",
    "\n",
    "Tables of forecast time values and filenames are used for interpolating the \"accumulation\" variables, such as snow and precipitation. This should be done after an entire year's worth of data has successfully copied from `$ARCHIVE` to scratch space, because this table will be referenced for the filepaths and timestamp information to re-stack. Plus this step utilizes the compute nodes which cannot see `$ARCHIVE`.\n",
    "\n",
    "*Note to internal SNAP users - it is currently unknown why there is an \"accumulation\" fix needed at all. There could be some info on this lurking somewhere.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b987d60b-6478-4dbc-bbd1-b1804ff8d5dd",
   "metadata": {},
   "source": [
    "Create the slurm script for getting the forecast times:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f97c9156-03ed-4503-bad9-525571df1ece",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "enter number of CPUs to use: 24\n",
      "Enter name of compute partition to use: t1small\n"
     ]
    }
   ],
   "source": [
    "ncpus = input(\"enter number of CPUs to use:\")\n",
    "partition = input(\"Enter name of compute partition to use:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b217bdb6-5959-468c-a220-02f39b6c9ee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forecast times slurm commands written to /center1/DYNDOWN/kmredilla/wrf_data/slurm/get_forecast_times_ccsm_hist.slurm\n"
     ]
    }
   ],
   "source": [
    "# since this is only done once for a group with all files, only need to specify group (no year(s))\n",
    "sbatch_fp = slurm_dir.joinpath(f\"get_forecast_times_{group}.slurm\")\n",
    "sbatch_out_fp = slurm_dir.joinpath(f\"get_forecast_times_{group}_%j.out\")\n",
    "wrf_scratch_dir = raw_scratch_dir.joinpath(group)\n",
    "sbatch_head = main.make_sbatch_head(slurm_email, partition, conda_init_script, ap_env)\n",
    "main.write_sbatch_forecast_times(sbatch_fp, sbatch_out_fp, wrf_scratch_dir, anc_dir, forecast_times_script, ncpus, sbatch_head)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b11ce6-ea59-4b52-a990-ccac79787df4",
   "metadata": {},
   "source": [
    "Submit the script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f20b003b-0bbf-4754-aa66-92159835efe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes > 30 minutes to run on Chinook\n",
    "job_id = main.submit_sbatch(sbatch_fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a503f512-ef61-499b-aca5-6f33b547d40c",
   "metadata": {},
   "source": [
    "### 1.2.2 - Ensure ancillary WRF files present\n",
    "\n",
    "The re-stacking will rely on a monthly WRF output file as a template for consistent metadata, and a WRF geogrid data file for determining correct spatial projection information, and for correctly rotating wind data. Make sure that both are present in the `anc_dir` directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8a6e7e0-529d-4e72-b51b-916a66d7aa5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "\n",
    "if not template_fp.exists():\n",
    "    shutil.copy(\"/archive/DYNDOWN/DIONE/pbieniek/gfdl/hist/monthly/monthly_PCPT-gfdlh.nc\", template_fp)\n",
    "    \n",
    "if not geogrid_fp.exists():\n",
    "    # the original location of this file is not known, in case it is ever deleted\n",
    "    #  from this source location it might still be available on Poseidon at \n",
    "    #  /workspace/Shared/Tech_Projects/wrf_data/project_data/ancillary_wrf_constants/geo_em.d01.nc\n",
    "    shutil.copy(\"/import/SNAP/wrf_data/project_data/ancillary_wrf_constants/geo_em.d01.nc\", geogrid_fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249de8e9-ba9d-4444-943f-3cf79a88c16d",
   "metadata": {},
   "source": [
    "### 1.2.3 - Run the restacking with slurm\n",
    "\n",
    "Make the slurm scripts for re-stacking data for a particular variable and year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "a6e59b27-d96f-44a6-b391-64a23e9c3b6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter name of WRF variable to re-stack (leave blank for all): PCPT\n",
      "Enter number of CPUs to use: 24\n",
      "Enter name of compute partition to use: t1small\n",
      "Enter year to work on (leave blank for all): 1970\n"
     ]
    }
   ],
   "source": [
    "varnames = input(\"Enter name of WRF variable to re-stack (leave blank for all):\") \n",
    "if varnames == \"\":\n",
    "    varnames = list(luts.var_attrs.keys())\n",
    "else:\n",
    "    varnames = [varnames]\n",
    "ncpus = input(\"Enter number of CPUs to use:\")\n",
    "partition = input(\"Enter name of compute partition to use:\")\n",
    "years = input(\"Enter year to work on (leave blank for all):\")\n",
    "if years == \"\":\n",
    "    years = luts.groups[group][\"years\"]\n",
    "else:\n",
    "    years = [years]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "ba350422-1f22-4551-bef4-80b2e9ebb7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sbatch_fps = []\n",
    "for year in years:\n",
    "    for varname in varnames:\n",
    "        # write to .slurm script\n",
    "        sbatch_fp = slurm_dir.joinpath(f\"restack_{group}_{year}_{varname}.slurm\")\n",
    "        # filepath for slurm stdout\n",
    "        sbatch_out_fp = slurm_dir.joinpath(f\"restack_{group}_{year}_{varname}_%j.out\")\n",
    "        sbatch_head = main.make_sbatch_head(\n",
    "            slurm_email, partition, conda_init_script, ap_env\n",
    "        )\n",
    "        \n",
    "        args = {\n",
    "            \"sbatch_fp\": sbatch_fp,\n",
    "            \"sbatch_out_fp\": sbatch_out_fp,\n",
    "            \"restack_script\": restack_script,\n",
    "            \"luts_fp\": luts_fp,\n",
    "            \"geogrid_fp\": geogrid_fp,\n",
    "            \"anc_dir\": anc_dir,\n",
    "            \"restacked_dir\": restack_scratch_dir,\n",
    "            \"group\": group,\n",
    "            \"fn_str\": luts.groups[group][\"fn_str\"],\n",
    "            \"year\": year,\n",
    "            \"varname\": varname,\n",
    "            \"ncpus\": ncpus,\n",
    "            \"sbatch_head\": sbatch_head,\n",
    "        }\n",
    "        \n",
    "        main.write_sbatch_restack(**args)\n",
    "        sbatch_fps.append(sbatch_fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "946b7ce0-cf4b-4a96-a3f0-035b8431f12c",
   "metadata": {},
   "source": [
    "Submit the `.slurm` scripts with `sbatch`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "2122d724-6884-4832-ac06-81ae2d9c659b",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_ids = [main.submit_sbatch(fp) for fp in sbatch_fps]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "384c3f7a-ccaf-4abd-81d2-8a8a938d157f",
   "metadata": {},
   "source": [
    "## 2 - Manage files\n",
    "\n",
    "### 2.1 - Move off scratch space\n",
    "\n",
    "### 2.2 - Copy to AWS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569efb49-d214-4ad3-856f-f2865ac7dfde",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
