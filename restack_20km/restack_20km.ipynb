{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8808d314-7b23-4581-8d0f-6dfb71154736",
   "metadata": {},
   "source": [
    "# 20 km WRF re-stacking pipeline\n",
    "\n",
    "This pipeline restructures the raw 20km WRF outputs that cover Alaska and the surrounding regions (created by P Bieniek) into more user-friendly files that can be easily imported into popular GIS software. This WRF dataset consists of hourly and daily outputs for one reanalysis, ERA-Interim, and two GCMs, GFDL-CM3, and NCAR-CCSM4. This pipeline is designed to be executed entirely from this notebook.\n",
    "\n",
    "This is a rather complicated SNAP data pipeline. It works on a large amount of data (~300 GB for a single model / scenario / year, so that's over 90 TB for $2 * 95 + 2 * 35 + 35$ model / scenario / year combinations), creates a large number of final data files (>10k), and makes use of slurm, specific directory structure / file management, and asyncronous execution ability (i.e. re-run certain steps, run steps for only certain variables, etc).\n",
    "\n",
    "### Processing strategy\n",
    "\n",
    "The default configuration for this pipeline is to process all available data - all year / variable / model / scenario combinations possible. However, a supplemental \"interactive\" notebook is provided as well for more fine-grained control, being set up to re-stack a single year's worth of data for a single variable / model / scenario combination. This is the lowest level of control offered. Find that notebook here - [restack_20km_interactive.ipynb](restack_20km_interactive.ipynb).\n",
    "\n",
    "As seen below, the input data are grouped by model and scenario names and are consistently structured - hourly and daily WRF model outputs grouped by yearly folders.\n",
    "\n",
    "Given the volume of this dataset, restacking all of the data in a single sequence does not make sense. Thus, execution of this notebook is broken up into what will be referred to as \"WRF groups\", which is simply the model + scenario combination being processed. This is the natural grouping given the way the dataset is organized on the current filesystem.\n",
    "\n",
    "## Pipeline\n",
    "\n",
    "The pipeline starts here, and is implemented for a single WRF group - the historical data of the NCAR-CCSM4 model - with descriptions of what is being done and why. Processing the remaining 4 WRF groups is done in an abbreviated fashion using the same code in this first section. Use this table of contents to jump to the section you wish to process:\n",
    "\n",
    "\n",
    "* [NCAR-CCSM4 Historical](#NCAR-CCSM4-Historical)\n",
    "* [GFDL-CM3 Historical](#GFDL-CM3-Historical)\n",
    "* [NCAR-CCSM4 RCP 8.5](#NCAR-CCSM4-RCP-8.5)\n",
    "* [GFDL-CM3 RCP 8.5](#GFDL-CM3-RCP-8.5)\n",
    "* [ERA-Interim](#ERA-Interim)\n",
    "\n",
    "First, we provide some background info on the file structure and pipeline.\n",
    "\n",
    "The WRF outputs of interest from different runs of model / scenario may be in separate directories, but there is consistency in file structure across all groups - all `hourly` and `daily` directories have annual subgroups consisting of the WRF outputs to be restacked:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "09e07936-3241-457a-a012-7c030ce89f78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[38;5;27m1970\u001b[0m/\n",
      "\u001b[38;5;27m1971\u001b[0m/\n",
      "\u001b[38;5;27m1972\u001b[0m/\n",
      "\u001b[38;5;27m1973\u001b[0m/\n",
      "\u001b[38;5;27m1974\u001b[0m/\n"
     ]
    }
   ],
   "source": [
    "ls /archive/DYNDOWN/DIONE/pbieniek/ccsm/hist/hourly | head -5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "76100442-3e72-497e-9387-11d9d9c1c7f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;27m2003\u001b[0m/\n",
      "\u001b[38;5;27m2004\u001b[0m/\n",
      "\u001b[38;5;27m2005\u001b[0m/\n",
      "nohup.out\n",
      "\u001b[38;5;34morgdata.sh\u001b[0m*\n",
      "\u001b[m"
     ]
    }
   ],
   "source": [
    "ls /archive/DYNDOWN/DIONE/pbieniek/ccsm/hist/hourly/ | tail -6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "60a75bd5-98b6-42af-8934-c008bafd896d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[38;5;34mdailylog.out\u001b[0m*\n",
      "\u001b[38;5;34mWRFDS_d01.1979-01-01_00.nc\u001b[0m*\n",
      "\u001b[38;5;34mWRFDS_d01.1979-01-01_01.nc\u001b[0m*\n",
      "\u001b[38;5;34mWRFDS_d01.1979-01-01_02.nc\u001b[0m*\n",
      "\u001b[38;5;34mWRFDS_d01.1979-01-01_03.nc\u001b[0m*\n",
      "ls: write error\n"
     ]
    }
   ],
   "source": [
    "ls /archive/DYNDOWN/DIONE/pbieniek/ccsm/hist/hourly/1979 | head -5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ac64b4-6808-48a2-9b07-04f7d5450a51",
   "metadata": {},
   "source": [
    "This structure applies for all outputs, and exists for the following model / scenario / year combinations:\n",
    "\n",
    "* ERA-Interim\n",
    "    * \"historical\": 1979-2015\n",
    "* GFDL-CM3\n",
    "    * historical: 1970-2006\n",
    "    * RCP 8.5: 2006-2100\n",
    "* NCAR-CCSM4\n",
    "    * historical: 1970-2005\n",
    "    * RCP 8.5: 2005-2100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b91d0803-1d11-420c-87cd-e2a68afd5248",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### System\n",
    "\n",
    "This pipeline is being developed on the Chinook cluster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09b5fef0-7edc-420f-a9b2-e43d858a7f4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linux chinook00.rcs.alaska.edu 2.6.32-754.35.1.el6.61015g0000.x86_64 #1 SMP Mon Dec 21 12:41:07 EST 2020 x86_64 x86_64 x86_64 GNU/Linux\n"
     ]
    }
   ],
   "source": [
    "!uname -a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553b2acb-eb3a-474f-9ace-0a5cf7132d0f",
   "metadata": {},
   "source": [
    "This pipeline makes use of slurm and multiple cores / compute nodes for processing in reasonable time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "381a1c57-bcad-4f25-8c56-ebf2367b1541",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "slurm 19.05.7\n"
     ]
    }
   ],
   "source": [
    "!sinfo -V"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41bcaca6-33c8-4485-9b69-52ff4a380746",
   "metadata": {},
   "source": [
    "#### Execution\n",
    "\n",
    "This notebook should be executed on a per-section basis to process the entire dataset. In other words, don't expect to run this entire notebook at once! The preferred workflow should be to complete a single WRF group-based section, commit changes which serves to document the processing done, and move on to the next section.\n",
    "\n",
    "To process only subsets of the target dataset, which might be done for fixing an issue or re-processing some failed runs, use the [interactive restack_20km notebook](restack_20km_interactive.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98bab88-a2ff-4e64-8978-c4d7c8eb69a1",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Environment\n",
    "\n",
    "Instead of relying on environment variables, this pipeline utilizes user-supplied parameters specified in the cells of this notebook by simply assigning values to variables prior to executing any processing code cells.\n",
    "\n",
    "The following variables are used throughout each singular run of the pipeline (i.e., each section of this notebook) and so will be set in each section:\n",
    "\n",
    "* `base_dir` - Full path to the directory that will contain all ancillary and intermediate files that will be kept, such as scripts for slurm / `sbatch`\n",
    "* `output_dir` - Full path to the directory that will contain the final output data (will be the same as `base_dir` here but specified separately for consistency with other SNAP pipelines)\n",
    "* `scratch_dir` - Full path to the scratch directory that raw WRF outputs will be copied to prior to processing them\n",
    "    * This pipelines works with WRF outputs that are on a mounted file system, and so can be copied over to scratch space and removed when done to improve IO and avoid the need to keep them in the `base_dir`.\n",
    "* `group`: Encoded value specifying the WRF group being worked on, which is just a combination of the model and scenario (or just model, in terms of ERA-Interim).  One of [`era_interim`, `gfdl_hist`, `ccsm_hist`, `gfdl_rcp85`, `ccsm_rcp85`]. The correct directory to look in is then imported from the `luts.py` file.\n",
    "* `slurm_email` - String containing email address to use for failed slurm notifications\n",
    "* `conda_init_script` - This is currently specific to Chinook. This is the path to a script that contains commands for initializing the shells on the compute nodes to use `conda activate`, has the typical commands seen in `~/.bashrc` after installing conda:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "841e47c7-1503-4d1b-9ccc-f8babce63dba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/bin/bash\n",
      "\n",
      "# >>> conda initialize >>>\n",
      "# !! Contents within this block are managed by 'conda init' !!\n",
      "__conda_setup=\"$('/home/kmredilla/miniconda3/bin/conda' 'shell.bash' 'hook' 2> /dev/null)\"\n",
      "if [ $? -eq 0 ]; then\n",
      "    eval \"$__conda_setup\"\n",
      "else\n",
      "    if [ -f \"/home/kmredilla/miniconda3/etc/profile.d/conda.sh\" ]; then\n",
      "        . \"/home/kmredilla/miniconda3/etc/profile.d/conda.sh\"\n",
      "    else\n",
      "        export PATH=\"/home/kmredilla/miniconda3/bin:$PATH\"\n",
      "    fi\n",
      "fi\n",
      "unset __conda_setup\n",
      "# <<< conda initialize <<< \n",
      "\n"
     ]
    }
   ],
   "source": [
    "cat ~/init_conda.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b705d6-6b4a-492c-a474-63769b8211c1",
   "metadata": {},
   "source": [
    "With that, we are ready to run the pipeline for a single WRF group.\n",
    "\n",
    "# NCAR-CCSM4 Historical\n",
    "\n",
    "## 0 - Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd3db9a-84b8-495d-be41-49eedeec70c3",
   "metadata": {},
   "source": [
    "Set the \"user\" parameters - things that will likely change between different SNAP users running this pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e5b3fe8a-407b-43fb-a210-7b03d283ab88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scratch space where data will be copied for performant reading / writing\n",
    "scratch_dir = \"/center1/DYNDOWN/kmredilla/wrf_data\"\n",
    "slurm_email = \"kmredilla@alaska.edu\"\n",
    "conda_init_script = \"/home/kmredilla/init_conda.sh\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "433aee8a-52ee-40f0-9d47-74240b903d35",
   "metadata": {},
   "source": [
    "Set up imports and remaining relevant file and directory paths:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9414518-e855-49c0-8965-4e5fbddb14cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from multiprocessing import Pool\n",
    "from pathlib import Path\n",
    "import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "# codebase\n",
    "import luts\n",
    "import restack_20km as main\n",
    "\n",
    "\n",
    "base_dir = Path(\"/import/SNAP/wrf_data/project_data/wrf_data\")\n",
    "anc_dir = base_dir.joinpath(\"ancillary\")\n",
    "anc_dir.mkdir(exist_ok=True)\n",
    "# monthly WRF file to serve as template\n",
    "template_fp = anc_dir.joinpath(\"monthly_PCPT-gfdlh.nc\")\n",
    "# WRF geogrid file for correctly projecting data and rotating wind data\n",
    "geogrid_fp = anc_dir.joinpath(\"geo_em.d01.nc\")\n",
    "# final output directory for data\n",
    "output_dir = Path(\"/import/SNAP/wrf_data/project_data/wrf_data/restacked\")\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "scratch_dir = Path(scratch_dir)\n",
    "# where raw wrf outputs will be copied on scratch\n",
    "raw_scratch_dir = scratch_dir.joinpath(\"raw\")\n",
    "raw_scratch_dir.mkdir(exist_ok=True)\n",
    "# where initially restacked data will be stored on scratch_space\n",
    "restack_scratch_dir = scratch_dir.joinpath(\"restacked\")\n",
    "restack_scratch_dir.mkdir(exist_ok=True)\n",
    "slurm_dir = base_dir.joinpath(\"slurm\")\n",
    "slurm_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# this env var is always defined if notebook started with anaconda-project run\n",
    "project_dir = Path(os.getenv(\"PROJECT_DIR\"))\n",
    "ap_env = project_dir.joinpath(\"envs/default\")\n",
    "# cp_script = project_dir.joinpath(\"restack_20km/mp_cp.py\") not used on Chinook, $ARCHIVE not accessible from compute nodes\n",
    "restack_script = project_dir.joinpath(\"restack_20km/restack.py\")\n",
    "forecast_times_script = project_dir.joinpath(\"restack_20km/forecast_times.py\")\n",
    "luts_fp = project_dir.joinpath(\"restack_20km/luts.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "403f4d6c-88d3-4733-bf43-d365efbf23e8",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1 - Re-stack data and improve the file structure\n",
    "\n",
    "This is the main lift of the pipeline and it applies to a single WRF group (again, \"group\" meaning a specific model / scenario combination) for any variables and years specified. It re-stacks the WRF outputs, which means extracting the data for all variables in a single hourly WRF file and combining them into new files grouped by variable and year. It then assigns useful metadata and restructures the files to achieve greater usability (note - this was previously a separate step, but the storage of essentially duplicate intermediate data was not efficient).\n",
    "\n",
    "As mentioned above, this pipeline is currently configured to run for all potential combinations of variables / years for each group. This section will demonstrate execution of all the processing steps required to re-stack one single WRF group, NCAR-CCSM4 historical, and then will proceed to string them all together for processing the remaining WRF groups."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ebb316-9827-4495-bb0a-996d59194f14",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 1.1 - Copy WRF data to scratch space\n",
    "\n",
    "If not present on the filesystem (as is the case at the time of developing the current code) then the WRF data need to be copied over. This is done from tape storage if working on Chinook.\n",
    "\n",
    "This step will copy the annual subdirectory(ies) containing the WRF outputs for all specified years to scratch space for efficient reading. Given the location of the source data on $ARCHIVE, which requires files to be brought back online from tape to read them, this step is very time consuming.\n",
    "\n",
    "There are two steps required here:\n",
    "1. \"stage\" the files that are on tape storage - i.e., read them from tape to a temporary spot\n",
    "2. copy the files to scratch space for persistence\n",
    "\n",
    "It takes a very long to to `batch_stage` an entire group directory. This does not make for a useful experience when trying to execute this pipeline. Given that this transfer needs to be initiated from a login node*, it will be useful to have commands that \n",
    "1) determine what files (years) are missing from the scratch space\n",
    "2) checks to see what of those necessary files (years) are currently \"offline\" (i.e., in tape storage only and not available for immediate copy)\n",
    "3) copies files (years) that are online \n",
    "\n",
    "\\* $ARCHIVE is only visible to the login nodes, so this task cannot be split into subtasks for the compute nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b10fcc7-8643-415b-b0c6-42e52c984ab6",
   "metadata": {},
   "source": [
    "#### 1.1.1 - Specify the WRF group and years to be processed\n",
    "\n",
    "Set the parameters for processing. The `wrf_dir` specifies the path to the WRF group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0256af0-bd62-4d3f-8c1b-e2381439586f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# job parameters\n",
    "wrf_dir = Path(\"/archive/DYNDOWN/DIONE/pbieniek/ccsm/hist/hourly\")\n",
    "group = \"ccsm_hist\"\n",
    "# years = [2004, 2005]\n",
    "# to specify all years:\n",
    "years = luts.groups[group][\"years\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "380d6da3-2621-4e9b-9d87-3d8e64d8aefb",
   "metadata": {},
   "source": [
    "#### 1.1.2 - Stage the files to be processed\n",
    "\n",
    "Staging all files for a particular WRF group can be done with a single command on the above `wrf_dir`, e.g.:\n",
    "\n",
    "```\n",
    "batch_stage -r /archive/DYNDOWN/DIONE/pbieniek/ccsm/hist/hourly\n",
    "```\n",
    "\n",
    "However, the cell below will execute the staging for all supplied years. Once this command finishes, then proceed to copying the files. This can take a while."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d411f69-99ed-4928-97f5-3d5d786d449c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from subprocess import check_output\n",
    "\n",
    "\n",
    "stdout = []\n",
    "for year in years:\n",
    "    stage_dir = wrf_dir.joinpath(str(year))\n",
    "    out = check_output([\"batch_stage\", \"-r\", stage_dir])\n",
    "    stdout.append(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8823fcdd-c823-4a3f-ace0-b8f4d325e72f",
   "metadata": {},
   "source": [
    "**Note** - This step should always be done if the data are on `$ARCHIVE` on Chinook, as it should be much more efficient that running the copy without staged (> 3 hours to do a year without staging, ~45 minutes when staged (using 20 cores))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2468404-cfec-4756-8a8b-be0c7c5409f9",
   "metadata": {},
   "source": [
    "Check that all requested files actually staged:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8033f0ce-0b2c-4cee-9453-8fd439c78000",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requested years: [1970 1971 1972 1973 1974 1975 1976 1977 1978 1979 1980 1981 1982 1983\n",
      " 1984 1985 1986 1987 1988 1989 1990 1991 1992 1993 1994 1995 1996 1997\n",
      " 1998 1999 2000 2001 2002 2003 2004 2005]\n",
      "All files are staged\n",
      "CPU times: user 86 ms, sys: 102 ms, total: 188 ms\n",
      "Wall time: 2.42 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time unstaged_fps = main.check_staged(wrf_dir, years)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c6c050-e596-459b-a87d-970ac95b8ef5",
   "metadata": {},
   "source": [
    "Ensure yearly subdirectories are present before starting the copying:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a0986b7-e3b2-4d26-befc-77fcd2d751a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "main.make_yearly_scratch_dirs(group, years, raw_scratch_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ffaf2f-b04e-4254-907b-c49889b88a8d",
   "metadata": {},
   "source": [
    "#### 1.1.3 - Copy staged files to `scratch_dir`\n",
    "\n",
    "Iterate over years and copy the files in parallel with `multiprocesing.Pool`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b6ad9d4c-6fae-4c35-8062-213fcc7796e5",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4ff6a4835574ac7955576d608a9247c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Copying files for 20 years:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88921a296189425380a013e6e43ae7f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Year: 1984:   0%|          | 0/8760 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7db1276b800d4311adf34c728b977c47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Year: 1985:   0%|          | 0/8760 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94bf4df13c2b4bef822cbfa67676f7d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Year: 1986:   0%|          | 0/8760 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b87973fccd7b41489de185e17f037c01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Year: 1987:   0%|          | 0/8760 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87f3d00c282248d2bfefc6a7f7a100f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Year: 1988:   0%|          | 0/8760 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c191d955970c4e06a83d862ab0818182",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Year: 1989:   0%|          | 0/8760 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "042b1eef7fb84e429fc2daa2c9584114",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Year: 1990:   0%|          | 0/8760 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec3383bdbc6f4ec1bcc3cbb5f6573192",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Year: 1991:   0%|          | 0/8760 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73996865286d4addb37e1a5ad2322d8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Year: 1992:   0%|          | 0/8760 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3843b0af4b2c40439e5ca417c50bde4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Year: 1993:   0%|          | 0/8760 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d677315004d45e5877bbed00f4f84d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Year: 1994:   0%|          | 0/8760 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d52f1447055745ff92bd33633bba25fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Year: 1995:   0%|          | 0/8760 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "129640ca5caa49a59cee4df41db44518",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Year: 1996:   0%|          | 0/8760 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f225873d73c43cabc5395495ce37469",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Year: 1997:   0%|          | 0/8760 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "900d100554b94d42abc8791561d92fca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Year: 1998:   0%|          | 0/8760 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f3d1f07a6284414bcf3d6111c8fb076",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Year: 1999:   0%|          | 0/8760 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a95738288a04648b2a1cb31658cc9b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Year: 2000:   0%|          | 0/8760 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8410e94e25ea47aca80e22ae0826ade5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Year: 2001:   0%|          | 0/8760 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50db7453f7f94fb6af4a65ed7ba3d0ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Year: 2002:   0%|          | 0/8760 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b591025f5bc4e89bdf04ef20f6d96ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Year: 2003:   0%|          | 0/8760 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ncpus = 20\n",
    "clobber = \"all\"\n",
    "\n",
    "\n",
    "group_dir = raw_scratch_dir.joinpath(group)\n",
    "for year in tqdm(years, total=len(years), desc=f\"Copying files for {len(years)} years\"):\n",
    "    src_dir = wrf_dir.joinpath(str(year))\n",
    "    dst_dir = group_dir.joinpath(str(year))\n",
    "    # set third arg to False for no-clobber\n",
    "    args = [(fp, dst_dir.joinpath(fp.name), clobber) for fp in src_dir.glob(\"*.nc\")]\n",
    "    \n",
    "    with Pool(ncpus) as pool:\n",
    "        out = [out for out in tqdm(pool.imap(main.sys_copy, args), total=len(args), desc=f\"Year: {year}\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f5b910-3c72-4cfa-b750-80c493cca5f3",
   "metadata": {},
   "source": [
    "##### Check progress of copying to scratch\n",
    "\n",
    "Run the cell below to check the progress of the copy for the current arguments (this can take a while):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "017805fb-4dd1-4104-8ed7-e5cee73814ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All years present on scratch space\n"
     ]
    }
   ],
   "source": [
    "# modify this to show what entire years are present in scratch_dir\n",
    "\n",
    "wrf_fps, existing_scratch_fps = main.check_raw_scratch(wrf_dir, group, years, raw_scratch_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3659ed14-df71-462d-91e9-295d0a9a096f",
   "metadata": {},
   "source": [
    "This cell below can be used as a quick check to identify any files that didn't copy properly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "39b32e46-b334-407b-abe2-3af75eeac97c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "flag_fps = []\n",
    "\n",
    "for year in years:\n",
    "    year_scratch_dir = raw_scratch_dir.joinpath(group, year)\n",
    "    flag_fps.extend(check_scratch_file_sizes(year_scratch_dir, ncpus=20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787302a3-70fb-4ef8-8d8c-ce9df79d4731",
   "metadata": {},
   "source": [
    "Then, re-copy any missing files derived from that check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b109d9bf-8206-4f16-b730-0215310ba779",
   "metadata": {},
   "outputs": [],
   "source": [
    "main.recopy_raw_scratch_files(flag_fps, wrf_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "477344e8-c055-4467-a512-9a3a5f2d0e15",
   "metadata": {},
   "source": [
    "**Note** - if there is a large number of missing files, it might be more efficient to use the intial section above for copying in batch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a25943f1-9c55-48a7-a153-58b94cd8ba26",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 1.2 - Restack the data\n",
    "\n",
    "Now that the WRF outputs are available on the scratch filesystem for faster access, execute the restacking script on all variables of interest."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f2b1abb-4aea-4627-aea6-6ab418f79e7f",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 1.2.1 - Make forecast time table\n",
    "\n",
    "Tables of forecast time values and filenames are used for interpolating the \"accumulation\" variables, such as snow and precipitation. This should be done after an entire year's worth of data has successfully copied from `$ARCHIVE` to scratch space, because this table will be referenced for the filepaths and timestamp information to re-stack. Plus this step utilizes the compute nodes which cannot see `$ARCHIVE`.\n",
    "\n",
    "**Note** - it is currently unknown why there is an \"accumulation fix\" needed at all. There could be some info on this lurking somewhere."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b987d60b-6478-4dbc-bbd1-b1804ff8d5dd",
   "metadata": {},
   "source": [
    "Create the slurm script for getting the forecast times:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f97c9156-03ed-4503-bad9-525571df1ece",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "enter number of CPUs to use: 24\n",
      "Enter name of compute partition to use: t1small\n"
     ]
    }
   ],
   "source": [
    "ncpus = input(\"enter number of CPUs to use:\")\n",
    "partition = input(\"Enter name of compute partition to use:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b217bdb6-5959-468c-a220-02f39b6c9ee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forecast times slurm commands written to /center1/DYNDOWN/kmredilla/wrf_data/slurm/get_forecast_times_ccsm_hist.slurm\n"
     ]
    }
   ],
   "source": [
    "# since this is only done once for a group with all files, only need to specify group (no year(s))\n",
    "sbatch_fp = slurm_dir.joinpath(f\"get_forecast_times_{group}.slurm\")\n",
    "sbatch_out_fp = slurm_dir.joinpath(f\"get_forecast_times_{group}_%j.out\")\n",
    "wrf_scratch_dir = raw_scratch_dir.joinpath(group)\n",
    "sbatch_head = main.make_sbatch_head(slurm_email, partition, conda_init_script, ap_env)\n",
    "main.write_sbatch_forecast_times(sbatch_fp, sbatch_out_fp, wrf_scratch_dir, anc_dir, forecast_times_script, ncpus, sbatch_head)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b11ce6-ea59-4b52-a990-ccac79787df4",
   "metadata": {},
   "source": [
    "Submit the script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f20b003b-0bbf-4754-aa66-92159835efe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes > 30 minutes to run on Chinook\n",
    "job_id = main.submit_sbatch(sbatch_fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a503f512-ef61-499b-aca5-6f33b547d40c",
   "metadata": {},
   "source": [
    "#### 1.2.2 - Ensure ancillary WRF geogrid file is present\n",
    "\n",
    "The re-stacking will rely on a WRF geogrid data file for determining correct spatial projection information, and for correctly rotating data for wind variables. Make sure that it is present in the `anc_dir` directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8a6e7e0-529d-4e72-b51b-916a66d7aa5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "\n",
    "if not geogrid_fp.exists():\n",
    "    # the original location of this file is not known, in case it is ever deleted\n",
    "    #  from this source location it might still be available on Poseidon at \n",
    "    #  /workspace/Shared/Tech_Projects/wrf_data/project_data/ancillary_wrf_constants/geo_em.d01.nc\n",
    "    shutil.copy(\"/import/SNAP/wrf_data/project_data/ancillary_wrf_constants/geo_em.d01.nc\", geogrid_fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249de8e9-ba9d-4444-943f-3cf79a88c16d",
   "metadata": {},
   "source": [
    "#### 1.2.3 - Run the restacking with slurm\n",
    "\n",
    "Make the slurm scripts for re-stacking data for a particular variable and year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a6e59b27-d96f-44a6-b391-64a23e9c3b6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter name of WRF variable to re-stack (leave blank for all): \n",
      "Enter number of CPUs to use (leave blank for 24 cores): \n",
      "Enter name of compute partition to use (leave blank for 't1small'): \n",
      "Enter year to work on (leave blank for all): \n"
     ]
    }
   ],
   "source": [
    "varnames = luts.varnames\n",
    "ncpus = 24\n",
    "partition = \"t1small\"\n",
    "years = luts.groups[group][\"years\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ba350422-1f22-4551-bef4-80b2e9ebb7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sbatch_fps = []\n",
    "year_str = f\"{years[0]}-{years[-1]}\"\n",
    "for varname in varnames:\n",
    "    # write to .slurm script\n",
    "    sbatch_fp = slurm_dir.joinpath(f\"restack_{group}_{year_str}_{varname}.slurm\")\n",
    "    # filepath for slurm stdout\n",
    "    sbatch_out_fp = slurm_dir.joinpath(f\"restack_{group}_{year_str}_{varname}_%j.out\")\n",
    "    sbatch_head = main.make_sbatch_head(\n",
    "        slurm_email, partition, conda_init_script, ap_env\n",
    "    )\n",
    "\n",
    "    args = {\n",
    "        \"sbatch_fp\": sbatch_fp,\n",
    "        \"sbatch_out_fp\": sbatch_out_fp,\n",
    "        \"restack_script\": restack_script,\n",
    "        \"luts_fp\": luts_fp,\n",
    "        \"geogrid_fp\": geogrid_fp,\n",
    "        \"anc_dir\": anc_dir,\n",
    "        \"restacked_dir\": restack_scratch_dir,\n",
    "        \"group\": group,\n",
    "        \"fn_str\": luts.groups[group][\"fn_str\"],\n",
    "        \"years\": years,\n",
    "        \"varname\": varname,\n",
    "        \"ncpus\": ncpus,\n",
    "        \"sbatch_head\": sbatch_head,\n",
    "    }\n",
    "\n",
    "    main.write_sbatch_restack(**args)\n",
    "    sbatch_fps.append(sbatch_fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "071ca8e4-e5cc-4a61-b2a5-fbc109094dea",
   "metadata": {},
   "source": [
    "Remove existing slurm output scripts if you fancy it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5833e296-5633-4dba-8bf0-b364072b4e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "for varname in varnames:\n",
    "    _ = [fp.unlink() for fp in list(slurm_dir.glob(f\"*_{varname}_*.out\"))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "946b7ce0-cf4b-4a96-a3f0-035b8431f12c",
   "metadata": {},
   "source": [
    "Submit the `.slurm` scripts with `sbatch`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2122d724-6884-4832-ac06-81ae2d9c659b",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_ids = [main.submit_sbatch(fp) for fp in sbatch_fps]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e32de250-9655-4d32-bd8c-7e79ad01eca7",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2 - Quality Check\n",
    "\n",
    "### 2.1 - Ensure that all files open and have consistent header info\n",
    "\n",
    "Check this using both `xarray` and GDAL bindings.\n",
    "\n",
    "Dimensions are the same\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f4b4a32-2d63-466c-90d4-9a1b52a45eef",
   "metadata": {},
   "source": [
    "### 2.2 - Ensure that restacked data match raw inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ba710c4-252f-431f-868f-5bb4135bf37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_wrf_fps = list(restack_scratch_dir.glob(\"*/*.nc\"))\n",
    "args = [(fp, raw_scratch_dir) for fp in all_wrf_fps]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d169cba4-e4e1-445b-8a7a-254cac5b296a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                                                    | 0/50 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "tic = time.perf_counter()\n",
    "\n",
    "with Pool(20) as pool:\n",
    "    new_rows = [\n",
    "        result for result in tqdm.tqdm(\n",
    "            pool.imap_unordered(main.validate_slice, args[:50]), total=len(args[:50]))\n",
    "    ]\n",
    "    \n",
    "print(f\"Time elapsed: {round((time.perf_counter() - tic) / 60)}m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3334e8-952b-4990-89bc-bdb2c20d017f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
