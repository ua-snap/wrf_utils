{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "403f4d6c-88d3-4733-bf43-d365efbf23e8",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Restack hourly 20km WRF outputs\n",
    "\n",
    "Now that the WRF outputs are available on the scratch filesystem for persistence and fast access, execute the restacking script on all variables of interest.\n",
    "\n",
    "This is the main lift of the pipeline and it applies to a single WRF group (again, \"group\" meaning a specific model / scenario combination) for any variables and years specified. It \"restacks\" the WRF outputs, which means extracting the data for all variables in a single hourly WRF file and combining them into new files grouped by variable and year. It then assigns useful metadata and restructures the files to achieve greater usability (note - this was previously a separate step, but the storage of essentially duplicate intermediate data was not efficient).\n",
    "\n",
    "As mentioned above, this pipeline is currently configured to run the restacking for all potential combinations of variables / years for each group.\n",
    "\n",
    "Set up the environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9192f84-6c27-4724-a865-b547836a9180",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from multiprocessing import Pool\n",
    "import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "# from tqdm.notebook import tqdm as nb_tqdm\n",
    "# codebase\n",
    "from config import *\n",
    "import luts\n",
    "import slurm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb38c43-2fec-4277-b1c8-b54a4d87738b",
   "metadata": {},
   "source": [
    "### 1 - Make forecast time table\n",
    "\n",
    "Tables of forecast time values and filenames are used for interpolating the \"accumulation\" variables, such as snow and precipitation. This should be done after an entire year's worth of data has successfully copied from `$ARCHIVE` to scratch space, because this table will be referenced for the filepaths and timestamp information to restack. Plus this step utilizes the compute nodes which cannot see `$ARCHIVE`.\n",
    "\n",
    "**Note** - it is currently unknown why there is an \"accumulation fix\" needed at all. There could be some info on this lurking somewhere.\n",
    "\n",
    "Create the slurm script for getting the forecast times:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b217bdb6-5959-468c-a220-02f39b6c9ee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forecast times slurm commands written to /center1/DYNDOWN/kmredilla/wrf_data/slurm/get_forecast_times_ccsm_hist.slurm\n"
     ]
    }
   ],
   "source": [
    "# since this is only done once for a group with all files, only need to specify group (no year(s))\n",
    "sbatch_fp = slurm_dir.joinpath(f\"get_forecast_times_{group}.slurm\")\n",
    "sbatch_out_fp = slurm_dir.joinpath(f\"get_forecast_times_{group}_%j.out\")\n",
    "wrf_scratch_dir = raw_scratch_dir.joinpath(group)\n",
    "sbatch_head = slurm.make_sbatch_head(slurm_email, partition, conda_init_script)\n",
    "slurm.write_sbatch_forecast_times(sbatch_fp, sbatch_out_fp, wrf_scratch_dir, anc_dir, forecast_times_script, ncpus, sbatch_head)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b11ce6-ea59-4b52-a990-ccac79787df4",
   "metadata": {},
   "source": [
    "Submit the script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f20b003b-0bbf-4754-aa66-92159835efe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes > 30 minutes to run on Chinook\n",
    "job_id = slurm.submit_sbatch(sbatch_fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249de8e9-ba9d-4444-943f-3cf79a88c16d",
   "metadata": {},
   "source": [
    "### 2 - Run the restacking with slurm\n",
    "\n",
    "Make the slurm scripts for restacking data for a particular variable and year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "ba350422-1f22-4551-bef4-80b2e9ebb7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "varnames = luts.varnames\n",
    "\n",
    "sbatch_fps = []\n",
    "year_str = f\"{years[0]}-{years[-1]}\"\n",
    "for varname in varnames:\n",
    "    # write to .slurm script\n",
    "    sbatch_fp = slurm_dir.joinpath(f\"restack_{group}_{year_str}_{varname}.slurm\")\n",
    "    # filepath for slurm stdout\n",
    "    sbatch_out_fp = slurm_dir.joinpath(f\"restack_{group}_{year_str}_{varname}_%j.out\")\n",
    "    sbatch_head = slurm.make_sbatch_head(\n",
    "        slurm_email, partition, conda_init_script\n",
    "    )\n",
    "\n",
    "    args = {\n",
    "        \"sbatch_fp\": sbatch_fp,\n",
    "        \"sbatch_out_fp\": sbatch_out_fp,\n",
    "        \"restack_script\": restack_script,\n",
    "        \"luts_fp\": luts_fp,\n",
    "        \"geogrid_fp\": geogrid_fp,\n",
    "        \"anc_dir\": anc_dir,\n",
    "        \"restacked_dir\": restack_scratch_dir,\n",
    "        \"group\": group,\n",
    "        \"fn_str\": luts.groups[group][\"fn_str\"],\n",
    "        \"years\": years,\n",
    "        \"varname\": varname,\n",
    "        \"ncpus\": ncpus,\n",
    "        \"sbatch_head\": sbatch_head,\n",
    "    }\n",
    "\n",
    "    slurm.write_sbatch_restack(**args)\n",
    "    sbatch_fps.append(sbatch_fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "071ca8e4-e5cc-4a61-b2a5-fbc109094dea",
   "metadata": {},
   "source": [
    "Remove existing slurm output scripts if you fancy it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "5833e296-5633-4dba-8bf0-b364072b4e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "for varname in varnames:\n",
    "    _ = [fp.unlink() for fp in list(slurm_dir.glob(f\"*{group}_{year_str}_{varname}_*.out\"))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "946b7ce0-cf4b-4a96-a3f0-035b8431f12c",
   "metadata": {},
   "source": [
    "Submit the `.slurm` scripts with `sbatch`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "2122d724-6884-4832-ac06-81ae2d9c659b",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_ids = [slurm.submit_sbatch(fp) for fp in sbatch_fps]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a32aa094-8201-4451-9d21-1fc43e44e32d",
   "metadata": {},
   "source": [
    "This should complete this step of the pipeline. Once the slurm jobs have all finished, proceed to resampling the restacked files to a daily resolution. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
