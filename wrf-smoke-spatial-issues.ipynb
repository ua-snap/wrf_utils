{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ISSUES WITH WRF-SMOKE NETCDF DATA\n",
    "\n",
    "__working_filename__: `/workspace/Shared/Users/malindgren/wrf_smoke/raw/wrfout_d01_2017-06-15_00:00:00`\n",
    "\n",
    "***things we know from the file itself and supplemental emails:***\n",
    "- **proj4 string:**\t`+proj=lcc +lat_1=65 +lat_2=65 +lat_0=65 +lon_0=-152 +R=6370000`\n",
    "- **GRID METADATA**\n",
    "    - GRID_ID:                         1\n",
    "    - PARENT_ID:                       0\n",
    "    - I_PARENT_START:                  1\n",
    "    - J_PARENT_START:                  1\n",
    "    - PARENT_GRID_RATIO:               1\n",
    "    - CEN_LAT:                         65.0\n",
    "    - CEN_LON:                         -152.0\n",
    "    - TRUELAT1:                        65.0\n",
    "    - TRUELAT2:                        65.0\n",
    "    - MOAD_CEN_LAT:                    65.0\n",
    "    - STAND_LON:                       -152.0\n",
    "    - POLE_LAT:                        90.0\n",
    "    - POLE_LON:                        0.0\n",
    "    - GMT:                             0.0\n",
    "    - JULYR:                           2017\n",
    "    - JULDAY:                          166\n",
    "    - MAP_PROJ:                        1\n",
    "    - MAP_PROJ_CHAR:                   Lambert Conformal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/malindgren/v3/lib/python3.5/site-packages/xarray/core/formatting.py:16: FutureWarning: The pandas.tslib module is deprecated and will be removed in a future version.\n",
      "  from pandas.tslib import OutOfBoundsDatetime\n"
     ]
    }
   ],
   "source": [
    "# lets read in the file so we can examine some _stuff_ about it\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "fn = '/Volumes/Shared/Users/malindgren/wrf_smoke/raw/wrfout_d01_2017-06-15_00-00-00_ML.nc'\n",
    "ds = xr.open_dataset( fn )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We know (from Chris' email) that `XLONG` `XLAT` are the spatial dimensions we should use. *Note I am assuming these are in the source crs which was determined to be Lambert projection or WRF-GRID:01 (see above file metadata)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LONGITUDES ARE NOT REGULARLY SPACED:\n",
      "  longitude diff min: 0.0818023681640625\n",
      "  longitude diff max: 0.1420440673828125\n",
      "\n",
      "LATITUDES ARE REGULARLY SPACED:\n",
      "  latitude diff min: -0.01416015625\n",
      "  latitude diff max: 0.01416015625\n"
     ]
    }
   ],
   "source": [
    "# grab only a single 2D slice of the dimension.\n",
    "lons = ds[ 'XLONG' ][ 0 ]\n",
    "lats = ds[ 'XLAT' ][ 0 ]\n",
    "\n",
    "# resolution -- sort of hacky but should get us close if it is a regular grid (spoiler alert!: its not)\n",
    "lons_diff = np.diff( lons )\n",
    "print( 'LONGITUDES ARE NOT REGULARLY SPACED:' )\n",
    "print( '  longitude diff min: {}\\n  longitude diff max: {}\\n'.format( lons_diff.min(), lons_diff.max() ) )\n",
    "lats_diff = np.diff( lats )\n",
    "print( 'LATITUDES ARE REGULARLY SPACED:' )\n",
    "print( '  latitude diff min: {}\\n  latitude diff max: {}'.format( lats_diff.min(), lats_diff.max() ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### using what we know from the file we can make a very simplistic [affine transform](http://www.perrygeo.com/python-affine-transforms.html) for these data, which should get us somewhat close to where we should be in space.\n",
    "\n",
    "BUT as we have seen above the data are _not_ regularly spaced... So a common way of determining a resolution for the raster would be to take the `np.mean` of the non-regularly spaced longitudes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "origin from file: (-172.42266845703125,71.68465423583984)\n"
     ]
    }
   ],
   "source": [
    "lon_res = np.mean( lons_diff.ravel() )\n",
    "lat_res = lats_diff.ravel()[0] # grab the first one since this dimension is regularly spaced.\n",
    "\n",
    "import rasterio\n",
    "\n",
    "# grab the upper-left corner - which is where GDAL (and rasterio) reads from. aka the origin.\n",
    "ulx, uly = np.min( lons.data ), np.max( lats.data )\n",
    "print( 'origin from file: ({},{})'.format(ulx, uly) )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets look at the ulx,uly values from the above cell, while comparing to the `GCPs` which we also received from the data provider.\n",
    "\n",
    "`-gcp 0 0 -164.510605 57.652809 -gcp 0 298 -172.422668 70.599640 -gcp 298 0 -139.489395 57.652809 -gcp 298 298 -131.577332 70.599640`\n",
    "\n",
    "A quick look at cell 0,0 (which would be the origin of the file) in the above gcps shows: `-164.510605, 57.652809`\n",
    "This is obviously incorrect given the GDAL way of reading files, so we are not going to be able to use the row/col id's coupled with the x,y for the GCPs...  at least in the form it is now.  Also, according to Chris' most recent email, these GCP's characterize the corner points of the grid to be generated, which do not seem to match those in the `XLONG`/`XLAT` dimensions in the NetCDF file.\n",
    "\n",
    "It apears that in the above GCPs `-gcp 0 298 -172.422668 70.599640` appears to be closest to the upper-left corner that we want for our affine transform origin, based on the `XLONG`/`XLAT` dimensions in the file. From the file, however, our origin is `-172.42266845703125, 71.68465423583984`, which is fine for longitudes, but for latitudes it is _close_, but not exact. This means that the data we are pulling from the file are more likely the corner points and the gcps are located somewhere within the cell it is related to.\n",
    "\n",
    "Lets do a difference of the 2 latitude corners:\n",
    "\n",
    "`71.68465423583984 - 70.599640 = 1.08501423583985`\n",
    "\n",
    "Judging by the data coordinates, the data do not seem to have a resolution of >=1 degree (see spacing min/max above), so I am not sure what this GCP is in reference to...  Its not a centroid, which would be 1/2 the cell resolution in each direction, and it is not the upper-left corner which would mean it was the same as the max latitude in the NetCDF file, and it is not. Ultimately, something is incorrect in either the GCPs provided, _or_ the information in the NetCDF file. Hard to tell what is wrong.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "No such file or directory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-6e2c51ef9c0b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# read in the prepped file (generated earlier) where I have selected and re-stacked the data in a more sane way\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mout_ds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen_dataset\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m'/workspace/Shared/Users/malindgren/wrf_smoke/netcdf/wrfout_d01_PM2_5_DRY_2017-06-15_00-00-00.nc'\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mvariable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'PM2_5_DRY'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mcrs_proj4\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'+proj=lcc +lat_1=65 +lat_2=65 +lat_0=65 +lon_0=-152 +R=6370000'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/v3/lib/python3.5/site-packages/xarray/backends/api.py\u001b[0m in \u001b[0;36mopen_dataset\u001b[0;34m(filename_or_obj, group, decode_cf, mask_and_scale, decode_times, autoclose, concat_characters, decode_coords, engine, chunks, lock, cache, drop_variables)\u001b[0m\n\u001b[1;32m    289\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'netcdf4'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m             store = backends.NetCDF4DataStore(filename_or_obj, group=group,\n\u001b[0;32m--> 291\u001b[0;31m                                               autoclose=autoclose)\n\u001b[0m\u001b[1;32m    292\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'scipy'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m             store = backends.ScipyDataStore(filename_or_obj,\n",
      "\u001b[0;32m~/v3/lib/python3.5/site-packages/xarray/backends/netCDF4_.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, filename, mode, format, group, writer, clobber, diskless, persist, autoclose)\u001b[0m\n\u001b[1;32m    208\u001b[0m                                    \u001b[0mdiskless\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdiskless\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpersist\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpersist\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m                                    format=format)\n\u001b[0;32m--> 210\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopener\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_autoclose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mautoclose\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_isopen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/v3/lib/python3.5/site-packages/xarray/backends/netCDF4_.py\u001b[0m in \u001b[0;36m_open_netcdf4_group\u001b[0;34m(filename, mode, group, **kwargs)\u001b[0m\n\u001b[1;32m    183\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0mnetCDF4\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnc4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m     \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnc4\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mclose_on_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mnetCDF4/_netCDF4.pyx\u001b[0m in \u001b[0;36mnetCDF4._netCDF4.Dataset.__init__ (netCDF4/_netCDF4.c:13842)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mnetCDF4/_netCDF4.pyx\u001b[0m in \u001b[0;36mnetCDF4._netCDF4._ensure_nc_success (netCDF4/_netCDF4.c:12628)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: No such file or directory"
     ]
    }
   ],
   "source": [
    "# so in order to do something lets use the data \n",
    "# make an affine transform from the lons/lats\n",
    "affine = rasterio.transform.from_origin( ulx, uly, lon_res, lat_res )\n",
    "# affine = rasterio.transform.from_origin( -172.422668, 70.59963999, res[0], res[1] )\n",
    "\n",
    "# read in the prepped file (generated earlier) where I have selected and re-stacked the data in a more sane way\n",
    "out_ds = xr.open_dataset( '/workspace/Shared/Users/malindgren/wrf_smoke/netcdf/wrfout_d01_PM2_5_DRY_2017-06-15_00-00-00.nc' )\n",
    "variable = 'PM2_5_DRY'\n",
    "crs_proj4 = '+proj=lcc +lat_1=65 +lat_2=65 +lat_0=65 +lon_0=-152 +R=6370000'\n",
    "# convert proj4 to rasterio CRS mapping\n",
    "crs = rasterio.crs.CRS.from_string( crs_proj4 )\n",
    "\n",
    "# build some output metadata\n",
    "time, levels, height, width = out_ds[ variable ].shape\n",
    "meta = {'res':res, 'transform':affine, 'height':height, 'width':width, 'count':1, 'dtype':'float32', 'driver':'GTiff', 'compress':'lzw', 'crs':crs }\n",
    "\n",
    "# this will output the data in its 'raw' crs for examination with other data before we try a reprojection.\n",
    "with rasterio.open( '/workspace/Shared/Users/malindgren/wrf_smoke/smoke_raw.tif', mode='w', **meta ) as out:\n",
    "    out.write( np.flipud( out_ds_level[ 23, ... ].data ), 1 )\n",
    "\n",
    "# ABOVE DATA IS INCORRECT...  SAME AS WHAT BOB WAS SEEING IN MapVenture... Not sure where to take this from here..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
